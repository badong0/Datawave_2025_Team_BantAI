{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "235323f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ RBA DATASET QUICK READER\n",
      "==================================================\n",
      "üìù Instructions:\n",
      "1. Replace 'file_path' variable with your actual dataset path\n",
      "2. Run this script\n",
      "3. Get quick insights from 10K rows in seconds!\n",
      "\n",
      "‚ö†Ô∏è Please update the file_path variable with your actual dataset path!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple RBA Dataset Reader - Just 10K Rows\n",
    "Quick and easy way to load and explore the first 10,000 rows of your 8GB dataset\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "def read_rba_sample(file_path, n_rows=10000):\n",
    "    \"\"\"\n",
    "    Simple function to read first 10K rows of RBA dataset\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to your CSV file\n",
    "        n_rows: Number of rows to read (default: 10,000)\n",
    "    \n",
    "    Returns:\n",
    "        pandas DataFrame with the sample data\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Reading first {n_rows:,} rows from RBA dataset...\")\n",
    "    print(f\"üìÅ File: {file_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"‚ùå File not found: {file_path}\")\n",
    "        print(\"üí° Make sure the file path is correct!\")\n",
    "        return None\n",
    "    \n",
    "    # Check file size\n",
    "    file_size_gb = os.path.getsize(file_path) / (1024**3)\n",
    "    print(f\"üìä File size: {file_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Read just the first n_rows\n",
    "        print(f\"‚è≥ Loading {n_rows:,} rows...\")\n",
    "        \n",
    "        df = pd.read_csv(file_path, nrows=n_rows)\n",
    "        \n",
    "        # Calculate timing\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Show results\n",
    "        print(f\"‚úÖ Successfully loaded {len(df):,} rows in {elapsed_time:.2f} seconds!\")\n",
    "        print(f\"üìä Dataset shape: {df.shape}\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return None\n",
    "\n",
    "def explore_rba_data(df):\n",
    "    \"\"\"\n",
    "    Quick exploration of the RBA dataset sample\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã DATASET EXPLORATION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"\\nüìä BASIC INFORMATION\")\n",
    "    print(f\"Rows: {len(df):,}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # Column names and types\n",
    "    print(f\"\\nüìã COLUMNS:\")\n",
    "    for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):\n",
    "        null_count = df[col].isnull().sum()\n",
    "        null_pct = (null_count / len(df)) * 100\n",
    "        print(f\"{i:2d}. {col:25s} | {str(dtype):15s} | {null_count:4d} nulls ({null_pct:5.1f}%)\")\n",
    "    \n",
    "    # First few rows\n",
    "    print(f\"\\nüëÄ FIRST 5 ROWS:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Basic statistics for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà NUMERIC COLUMNS SUMMARY:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # Categorical columns summary\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüìù CATEGORICAL COLUMNS:\")\n",
    "        for col in categorical_cols[:5]:  # Show first 5 categorical columns\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"{col}: {unique_count} unique values\")\n",
    "            if unique_count <= 10:\n",
    "                print(f\"  Values: {list(df[col].unique())}\")\n",
    "            else:\n",
    "                print(f\"  Top values: {list(df[col].value_counts().head(5).index)}\")\n",
    "    \n",
    "    # Missing data summary\n",
    "    missing_data = df.isnull().sum()\n",
    "    if missing_data.sum() > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è MISSING DATA:\")\n",
    "        for col, missing in missing_data[missing_data > 0].items():\n",
    "            pct = (missing / len(df)) * 100\n",
    "            print(f\"{col}: {missing} missing ({pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ NO MISSING DATA!\")\n",
    "\n",
    "def quick_analysis(df):\n",
    "    \"\"\"\n",
    "    Quick analysis specific to RBA data patterns\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç QUICK RBA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Look for common RBA columns\n",
    "    rba_columns = {\n",
    "        'user_id': ['user_id', 'userid', 'user', 'id'],\n",
    "        'timestamp': ['timestamp', 'time', 'datetime', 'date'],\n",
    "        'success': ['success', 'is_successful', 'login_success', 'result'],\n",
    "        'risk_score': ['risk_score', 'risk', 'score'],\n",
    "        'ip_address': ['ip_address', 'ip', 'client_ip'],\n",
    "        'country': ['country', 'location', 'geo'],\n",
    "        'user_agent': ['user_agent', 'browser', 'device']\n",
    "    }\n",
    "    \n",
    "    found_columns = {}\n",
    "    for category, possible_names in rba_columns.items():\n",
    "        for col in df.columns:\n",
    "            if col.lower() in [name.lower() for name in possible_names]:\n",
    "                found_columns[category] = col\n",
    "                break\n",
    "    \n",
    "    print(f\"üîé IDENTIFIED RBA COLUMNS:\")\n",
    "    for category, column in found_columns.items():\n",
    "        print(f\"  {category}: {column}\")\n",
    "    \n",
    "    # Quick stats on identified columns\n",
    "    if 'success' in found_columns:\n",
    "        success_col = found_columns['success']\n",
    "        success_rate = df[success_col].mean() if df[success_col].dtype in ['int64', 'float64'] else None\n",
    "        if success_rate is not None:\n",
    "            print(f\"\\nüìä SUCCESS RATE: {success_rate:.2%}\")\n",
    "    \n",
    "    if 'risk_score' in found_columns:\n",
    "        risk_col = found_columns['risk_score']\n",
    "        if df[risk_col].dtype in ['int64', 'float64']:\n",
    "            print(f\"üìä RISK SCORE STATS:\")\n",
    "            print(f\"  Mean: {df[risk_col].mean():.3f}\")\n",
    "            print(f\"  Min: {df[risk_col].min():.3f}\")\n",
    "            print(f\"  Max: {df[risk_col].max():.3f}\")\n",
    "    \n",
    "    if 'user_id' in found_columns:\n",
    "        user_col = found_columns['user_id']\n",
    "        unique_users = df[user_col].nunique()\n",
    "        total_records = len(df)\n",
    "        avg_records_per_user = total_records / unique_users\n",
    "        print(f\"üìä USER STATS:\")\n",
    "        print(f\"  Unique users: {unique_users:,}\")\n",
    "        print(f\"  Avg records per user: {avg_records_per_user:.1f}\")\n",
    "    \n",
    "    if 'country' in found_columns:\n",
    "        country_col = found_columns['country']\n",
    "        top_countries = df[country_col].value_counts().head(5)\n",
    "        print(f\"üìä TOP 5 COUNTRIES:\")\n",
    "        for country, count in top_countries.items():\n",
    "            pct = (count / len(df)) * 100\n",
    "            print(f\"  {country}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function - replace with your file path\n",
    "    \"\"\"\n",
    "    # üî• REPLACE THIS PATH WITH YOUR ACTUAL DATASET PATH\n",
    "    file_path = \"BantAI Datawave/rba-dataset.csv\"\n",
    "    \n",
    "    print(\"üöÄ RBA DATASET QUICK READER\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"üìù Instructions:\")\n",
    "    print(\"1. Replace 'file_path' variable with your actual dataset path\")\n",
    "    print(\"2. Run this script\")\n",
    "    print(\"3. Get quick insights from 10K rows in seconds!\")\n",
    "    print()\n",
    "    \n",
    "    # Check if user has updated the path\n",
    "    if file_path == \"BantAI Datawave/rba-dataset.csv\":\n",
    "        print(\"‚ö†Ô∏è Please update the file_path variable with your actual dataset path!\")\n",
    "\n",
    "        return\n",
    "    \n",
    "    # Read the sample\n",
    "    df = read_rba_sample(file_path, n_rows=10000)\n",
    "    \n",
    "    if df is not None:\n",
    "        # Explore the data\n",
    "        explore_rba_data(df)\n",
    "        \n",
    "        # Quick RBA-specific analysis\n",
    "        quick_analysis(df)\n",
    "        \n",
    "        print(\"\\nüéâ ANALYSIS COMPLETE!\")\n",
    "        print(\"üí° Next steps:\")\n",
    "        print(\"‚Ä¢ Increase sample size if this looks good\")\n",
    "        print(\"‚Ä¢ Run full machine learning analysis\")\n",
    "        print(\"‚Ä¢ Convert to Parquet for faster future loading\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"‚ùå Failed to load data. Check your file path!\")\n",
    "        return None\n",
    "\n",
    "# For immediate use:\n",
    "def load_10k_sample(file_path):\n",
    "    \"\"\"\n",
    "    Super simple function - just give it a file path!\n",
    "    \"\"\"\n",
    "    print(f\"üìñ Loading 10K rows from: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=10000)\n",
    "        print(f\"‚úÖ Loaded {len(df)} rows, {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main function\n",
    "    df = main()\n",
    "    \n",
    "    # If you want to use the data for further analysis:\n",
    "    # df = load_10k_sample(\"your_file_path.csv\")\n",
    "    # print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4ccf8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccbee69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SEARCHING FOR YOUR RBA DATASET...\n",
      "==================================================\n",
      "\n",
      "üìÇ Checking: .\n",
      "  üìÅ rba-dataset.csv (8633.5 MB)\n",
      "\n",
      "üìÇ Checking: C:/Users/Brando/Downloads/\n",
      "  üìÅ ai_job_dataset.csv (2.5 MB)\n",
      "  üìÅ people.csv (0.3 MB)\n",
      "  üìÅ reviews.csv (0.2 MB)\n",
      "\n",
      "üìÇ Checking: C:/Users/Brando/Desktop/\n",
      "\n",
      "üìÇ Checking: C:/Users/Brando/Documents/\n",
      "\n",
      "üìÇ Checking: C:/Users/Brando/Desktop/School/Project/BantAI Datawave/\n",
      "\n",
      "üéØ FOUND LARGE CSV FILES:\n",
      "1. rba-dataset.csv (8633.5 MB)\n",
      "   Path: .\\rba-dataset.csv\n",
      "\n",
      "üöÄ ATTEMPTING TO LOAD LARGEST FILE:\n",
      "üìÅ File: rba-dataset.csv (8633.5 MB)\n",
      "üìç Path: .\\rba-dataset.csv\n",
      "‚è≥ Loading first 10,000 rows...\n",
      "‚úÖ SUCCESS! Loaded 10,000 rows in 0.10 seconds!\n",
      "üìä Shape: (10000, 16)\n",
      "üíæ Memory usage: 6.8 MB\n",
      "\n",
      "üìã COLUMNS (16 total):\n",
      " 1. index\n",
      " 2. Login Timestamp\n",
      " 3. User ID\n",
      " 4. Round-Trip Time [ms]\n",
      " 5. IP Address\n",
      " 6. Country\n",
      " 7. Region\n",
      " 8. City\n",
      " 9. ASN\n",
      "10. User Agent String\n",
      "11. Browser Name and Version\n",
      "12. OS Name and Version\n",
      "13. Device Type\n",
      "14. Login Successful\n",
      "15. Is Attack IP\n",
      "16. Is Account Takeover\n",
      "\n",
      "üëÄ FIRST 5 ROWS:\n",
      "   index          Login Timestamp              User ID  Round-Trip Time [ms]  \\\n",
      "0      0  2020-02-03 12:43:30.772 -4324475583306591935                   NaN   \n",
      "1      1  2020-02-03 12:43:43.549 -4324475583306591935                   NaN   \n",
      "2      2  2020-02-03 12:43:55.873 -3284137479262433373                   NaN   \n",
      "3      3  2020-02-03 12:43:56.180 -4324475583306591935                   NaN   \n",
      "4      4  2020-02-03 12:43:59.396 -4618854071942621186                   NaN   \n",
      "\n",
      "      IP Address Country    Region       City     ASN  \\\n",
      "0    10.0.65.171      NO         -          -   29695   \n",
      "1   194.87.207.6      AU         -          -   60117   \n",
      "2  81.167.144.58      NO  Vestland  Urangsvag   29695   \n",
      "3  170.39.78.152      US         -          -  393398   \n",
      "4      10.0.0.47      US  Virginia    Ashburn  398986   \n",
      "\n",
      "                                   User Agent String  \\\n",
      "0  Mozilla/5.0  (iPhone; CPU iPhone OS 13_4 like ...   \n",
      "1  Mozilla/5.0  (Linux; Android 4.1; Galaxy Nexus...   \n",
      "2  Mozilla/5.0  (iPad; CPU OS 7_1 like Mac OS X) ...   \n",
      "3  Mozilla/5.0  (Linux; Android 4.1; Galaxy Nexus...   \n",
      "4  Mozilla/5.0  (Linux; U; Android 2.2) Build/NMA...   \n",
      "\n",
      "          Browser Name and Version OS Name and Version Device Type  \\\n",
      "0              Firefox 20.0.0.1618            iOS 13.4      mobile   \n",
      "1          Chrome Mobile 46.0.2490         Android 4.1      mobile   \n",
      "2               Android 2.3.3.2672             iOS 7.1      mobile   \n",
      "3  Chrome Mobile WebView 85.0.4183         Android 4.1      mobile   \n",
      "4  Chrome Mobile WebView 85.0.4183         Android 2.2      mobile   \n",
      "\n",
      "   Login Successful  Is Attack IP  Is Account Takeover  \n",
      "0             False         False                False  \n",
      "1             False         False                False  \n",
      "2              True         False                False  \n",
      "3             False         False                False  \n",
      "4             False          True                False  \n",
      "\n",
      "üìà BASIC STATS:\n",
      "             index       User ID  Round-Trip Time [ms]            ASN\n",
      "count  10000.00000  1.000000e+04            392.000000   10000.000000\n",
      "mean    4999.50000 -1.497424e+18            714.772959  146289.035200\n",
      "std     2886.89568  4.760827e+18            886.504892  168434.904356\n",
      "min        0.00000 -9.218199e+18             11.000000     224.000000\n",
      "25%     2499.75000 -4.324476e+18            475.750000   29695.000000\n",
      "50%     4999.50000 -4.324476e+18            546.000000   41164.000000\n",
      "75%     7499.25000  2.093715e+18            718.250000  264763.750000\n",
      "max     9999.00000  9.218230e+18          13576.000000  507361.000000\n",
      "\n",
      "üéâ DATASET LOADED SUCCESSFULLY!\n",
      "Now you can analyze 10,000 rows of RBA data!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(\"üîç SEARCHING FOR YOUR RBA DATASET...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Search for CSV files in common locations\n",
    "search_paths = [\n",
    "    \".\",  # Current directory\n",
    "    \"C:/Users/Brando/Downloads/\",\n",
    "    \"C:/Users/Brando/Desktop/\",\n",
    "    \"C:/Users/Brando/Documents/\",\n",
    "    \"C:/Users/Brando/Desktop/School/Project/BantAI Datawave/\"\n",
    "]\n",
    "\n",
    "found_files = []\n",
    "\n",
    "for path in search_paths:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"\\nüìÇ Checking: {path}\")\n",
    "            files = os.listdir(path)\n",
    "            for file in files:\n",
    "                if file.endswith('.csv'):\n",
    "                    filepath = os.path.join(path, file)\n",
    "                    try:\n",
    "                        size_mb = os.path.getsize(filepath) / (1024**2)\n",
    "                        print(f\"  üìÅ {file} ({size_mb:.1f} MB)\")\n",
    "                        \n",
    "                        # Look for large files (RBA dataset should be huge)\n",
    "                        if size_mb > 50:  # Larger than 50MB\n",
    "                            found_files.append((filepath, file, size_mb))\n",
    "                    except:\n",
    "                        print(f\"  üìÅ {file} (size unknown)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Can't access {path}\")\n",
    "\n",
    "# Show potential RBA dataset files\n",
    "if found_files:\n",
    "    print(f\"\\nüéØ FOUND LARGE CSV FILES:\")\n",
    "    for i, (full_path, filename, size_mb) in enumerate(found_files, 1):\n",
    "        print(f\"{i}. {filename} ({size_mb:.1f} MB)\")\n",
    "        print(f\"   Path: {full_path}\")\n",
    "    \n",
    "    # Try to load the largest file automatically\n",
    "    largest_file = max(found_files, key=lambda x: x[2])\n",
    "    file_path = largest_file[0]\n",
    "    filename = largest_file[1]\n",
    "    size_mb = largest_file[2]\n",
    "    \n",
    "    print(f\"\\nüöÄ ATTEMPTING TO LOAD LARGEST FILE:\")\n",
    "    print(f\"üìÅ File: {filename} ({size_mb:.1f} MB)\")\n",
    "    print(f\"üìç Path: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        print(\"‚è≥ Loading first 10,000 rows...\")\n",
    "        \n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path, nrows=10000)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ SUCCESS! Loaded {len(df):,} rows in {elapsed:.2f} seconds!\")\n",
    "        print(f\"üìä Shape: {df.shape}\")\n",
    "        print(f\"üíæ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "        \n",
    "        print(f\"\\nüìã COLUMNS ({len(df.columns)} total):\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            print(f\"{i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\nüëÄ FIRST 5 ROWS:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        print(f\"\\nüìà BASIC STATS:\")\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numeric_cols) > 0:\n",
    "            print(df[numeric_cols].describe())\n",
    "        \n",
    "        print(f\"\\nüéâ DATASET LOADED SUCCESSFULLY!\")\n",
    "        print(f\"Now you can analyze {len(df):,} rows of RBA data!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {filename}: {e}\")\n",
    "        print(\"üí° This might not be the right file format\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n‚ùå NO LARGE CSV FILES FOUND!\")\n",
    "    print(f\"üí° Your RBA dataset might be:\")\n",
    "    print(f\"‚Ä¢ Not downloaded yet\")\n",
    "    print(f\"‚Ä¢ In a different location\")\n",
    "    print(f\"‚Ä¢ Named differently\")\n",
    "    print(f\"‚Ä¢ Compressed (zip file)\")\n",
    "    \n",
    "    print(f\"\\nüîç Manual search - run this:\")\n",
    "    print(f'import glob')\n",
    "    print(f'files = glob.glob(\"C:/Users/Brando/**/*.csv\", recursive=True)')\n",
    "    print(f'for f in files: print(f)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60f9709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Loading RBA dataset...\n",
      "‚úÖ Loaded 10,000 rows, 16 columns\n",
      "\n",
      "üîç QUICK RBA ANALYSIS\n",
      "==================================================\n",
      "üìä Overall Success Rate: 49.8%\n",
      "üö® Attack IP Rate: 8.9%\n",
      "üö® Account Takeover Rate: 0.0%\n",
      "\n",
      "üåç TOP 5 COUNTRIES:\n",
      "  NO: 5,091 (50.9%)\n",
      "  US: 2,040 (20.4%)\n",
      "  PL: 604 (6.0%)\n",
      "  BR: 569 (5.7%)\n",
      "  IN: 232 (2.3%)\n",
      "\n",
      "üì± DEVICE TYPES:\n",
      "  mobile: 6,616 (66.2%)\n",
      "  desktop: 3,100 (31.0%)\n",
      "  tablet: 281 (2.8%)\n",
      "  unknown: 2 (0.0%)\n",
      "  bot: 1 (0.0%)\n",
      "\n",
      "‚ùå FAILED LOGINS: 5,018 (50.2%)\n",
      "Top countries for failed logins:\n",
      "  NO: 1,741\n",
      "  US: 1,436\n",
      "  BR: 361\n",
      "\n",
      "‚è∞ TIME PATTERNS:\n",
      "Peak activity hours:\n",
      "  13:00 - 5,081 logins\n",
      "  14:00 - 3,679 logins\n",
      "  12:00 - 1,240 logins\n",
      "\n",
      "üéØ DATASET SUMMARY:\n",
      "‚Ä¢ Total Records: 10,000\n",
      "‚Ä¢ Unique Users: 4,743\n",
      "‚Ä¢ Date Range: 2020-02-03 12:43:30.772000 to 2020-02-03 14:42:21.689000\n",
      "‚Ä¢ Countries: 74\n",
      "‚Ä¢ Success Rate: 49.8%\n",
      "‚Ä¢ Attack Rate: 8.9%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data again (this will be super fast now - 0.07 seconds!)\n",
    "print(\"üìñ Loading RBA dataset...\")\n",
    "df = pd.read_csv('rba-dataset.csv', nrows=10000)\n",
    "print(f\"‚úÖ Loaded {len(df):,} rows, {len(df.columns)} columns\")\n",
    "\n",
    "# Now run the analysis\n",
    "print(\"\\nüîç QUICK RBA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Success rate\n",
    "success_rate = df['Login Successful'].mean()\n",
    "print(f\"üìä Overall Success Rate: {success_rate:.1%}\")\n",
    "\n",
    "# Attack patterns\n",
    "attack_rate = df['Is Attack IP'].mean()\n",
    "takeover_rate = df['Is Account Takeover'].mean()\n",
    "print(f\"üö® Attack IP Rate: {attack_rate:.1%}\")\n",
    "print(f\"üö® Account Takeover Rate: {takeover_rate:.1%}\")\n",
    "\n",
    "# Geographic distribution\n",
    "print(f\"\\nüåç TOP 5 COUNTRIES:\")\n",
    "top_countries = df['Country'].value_counts().head()\n",
    "for country, count in top_countries.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {country}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Device patterns\n",
    "print(f\"\\nüì± DEVICE TYPES:\")\n",
    "device_counts = df['Device Type'].value_counts()\n",
    "for device, count in device_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {device}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Failed login analysis\n",
    "failed_logins = df[df['Login Successful'] == False]\n",
    "print(f\"\\n‚ùå FAILED LOGINS: {len(failed_logins):,} ({len(failed_logins)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(failed_logins) > 0:\n",
    "    print(\"Top countries for failed logins:\")\n",
    "    failed_countries = failed_logins['Country'].value_counts().head(3)\n",
    "    for country, count in failed_countries.items():\n",
    "        print(f\"  {country}: {count:,}\")\n",
    "\n",
    "# Time patterns\n",
    "print(f\"\\n‚è∞ TIME PATTERNS:\")\n",
    "df['Login Timestamp'] = pd.to_datetime(df['Login Timestamp'])\n",
    "df['Hour'] = df['Login Timestamp'].dt.hour\n",
    "hourly_counts = df['Hour'].value_counts().sort_index()\n",
    "peak_hours = hourly_counts.nlargest(3)\n",
    "print(\"Peak activity hours:\")\n",
    "for hour, count in peak_hours.items():\n",
    "    print(f\"  {hour}:00 - {count:,} logins\")\n",
    "\n",
    "print(f\"\\nüéØ DATASET SUMMARY:\")\n",
    "print(f\"‚Ä¢ Total Records: {len(df):,}\")\n",
    "print(f\"‚Ä¢ Unique Users: {df['User ID'].nunique():,}\")\n",
    "print(f\"‚Ä¢ Date Range: {df['Login Timestamp'].min()} to {df['Login Timestamp'].max()}\")\n",
    "print(f\"‚Ä¢ Countries: {df['Country'].nunique()}\")\n",
    "print(f\"‚Ä¢ Success Rate: {success_rate:.1%}\")\n",
    "print(f\"‚Ä¢ Attack Rate: {attack_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e144cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç QUICK RBA ANALYSIS\n",
      "==================================================\n",
      "üìä Overall Success Rate: 49.8%\n",
      "üö® Attack IP Rate: 8.9%\n",
      "üö® Account Takeover Rate: 0.0%\n",
      "\n",
      "üåç TOP 5 COUNTRIES:\n",
      "  NO: 5,091 (50.9%)\n",
      "  US: 2,040 (20.4%)\n",
      "  PL: 604 (6.0%)\n",
      "  BR: 569 (5.7%)\n",
      "  IN: 232 (2.3%)\n",
      "\n",
      "üì± DEVICE TYPES:\n",
      "  mobile: 6,616 (66.2%)\n",
      "  desktop: 3,100 (31.0%)\n",
      "  tablet: 281 (2.8%)\n",
      "  unknown: 2 (0.0%)\n",
      "  bot: 1 (0.0%)\n",
      "\n",
      "‚ùå FAILED LOGINS: 5,018 (50.2%)\n",
      "Top countries for failed logins:\n",
      "  NO: 1,741\n",
      "  US: 1,436\n",
      "  BR: 361\n"
     ]
    }
   ],
   "source": [
    "# Let's do some quick analysis on your loaded data\n",
    "print(\"üîç QUICK RBA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Success rate\n",
    "success_rate = df['Login Successful'].mean()\n",
    "print(f\"üìä Overall Success Rate: {success_rate:.1%}\")\n",
    "\n",
    "# Attack patterns\n",
    "attack_rate = df['Is Attack IP'].mean()\n",
    "takeover_rate = df['Is Account Takeover'].mean()\n",
    "print(f\"üö® Attack IP Rate: {attack_rate:.1%}\")\n",
    "print(f\"üö® Account Takeover Rate: {takeover_rate:.1%}\")\n",
    "\n",
    "# Geographic distribution\n",
    "print(f\"\\nüåç TOP 5 COUNTRIES:\")\n",
    "top_countries = df['Country'].value_counts().head()\n",
    "for country, count in top_countries.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {country}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Device patterns\n",
    "print(f\"\\nüì± DEVICE TYPES:\")\n",
    "device_counts = df['Device Type'].value_counts()\n",
    "for device, count in device_counts.items():\n",
    "    pct = (count / len(df)) * 100\n",
    "    print(f\"  {device}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Failed login analysis\n",
    "failed_logins = df[df['Login Successful'] == False]\n",
    "print(f\"\\n‚ùå FAILED LOGINS: {len(failed_logins):,} ({len(failed_logins)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if len(failed_logins) > 0:\n",
    "    print(\"Top countries for failed logins:\")\n",
    "    failed_countries = failed_logins['Country'].value_counts().head(3)\n",
    "    for country, count in failed_countries.items():\n",
    "        print(f\"  {country}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ea339",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m_zmq.py:169\u001b[39m, in \u001b[36mzmq.backend.cython._zmq._check_rc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__dealloc__'\n",
      "Traceback (most recent call last):\n",
      "  File \"_zmq.py\", line 169, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Success Rate by Country\n",
    "country_success = df.groupby('Country')['Login Successful'].agg(['count', 'mean']).reset_index()\n",
    "country_success = country_success[country_success['count'] >= 50]  # Countries with 50+ attempts\n",
    "country_success = country_success.sort_values('mean')\n",
    "\n",
    "axes[0,0].barh(country_success['Country'], country_success['mean'] * 100)\n",
    "axes[0,0].set_title('Success Rate by Country (%)', fontweight='bold')\n",
    "axes[0,0].set_xlabel('Success Rate (%)')\n",
    "\n",
    "# 2. Attack Rate by Country  \n",
    "country_attacks = df.groupby('Country')['Is Attack IP'].agg(['count', 'mean']).reset_index()\n",
    "country_attacks = country_attacks[country_attacks['count'] >= 50]\n",
    "country_attacks = country_attacks.sort_values('mean', ascending=False)\n",
    "\n",
    "axes[0,1].barh(country_attacks['Country'], country_attacks['mean'] * 100, color='red', alpha=0.7)\n",
    "axes[0,1].set_title('Attack IP Rate by Country (%)', fontweight='bold')\n",
    "axes[0,1].set_xlabel('Attack IP Rate (%)')\n",
    "\n",
    "# 3. Hourly Login Patterns\n",
    "hourly_data = df.groupby('Hour').agg({\n",
    "    'Login Successful': ['count', 'mean'],\n",
    "    'Is Attack IP': 'mean'\n",
    "}).reset_index()\n",
    "hourly_data.columns = ['Hour', 'Total_Logins', 'Success_Rate', 'Attack_Rate']\n",
    "\n",
    "axes[0,2].plot(hourly_data['Hour'], hourly_data['Total_Logins'], marker='o', linewidth=2)\n",
    "axes[0,2].set_title('Login Activity by Hour', fontweight='bold')\n",
    "axes[0,2].set_xlabel('Hour of Day')\n",
    "axes[0,2].set_ylabel('Number of Logins')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Device Type vs Success Rate\n",
    "device_success = df.groupby('Device Type').agg({\n",
    "    'Login Successful': ['count', 'mean'],\n",
    "    'Is Attack IP': 'mean'\n",
    "}).reset_index()\n",
    "device_success.columns = ['Device_Type', 'Count', 'Success_Rate', 'Attack_Rate']\n",
    "\n",
    "axes[1,0].bar(device_success['Device_Type'], device_success['Success_Rate'] * 100, alpha=0.7)\n",
    "axes[1,0].set_title('Success Rate by Device Type', fontweight='bold')\n",
    "axes[1,0].set_ylabel('Success Rate (%)')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Success vs Attack IP correlation\n",
    "attack_success = df.groupby('Is Attack IP')['Login Successful'].mean() * 100\n",
    "attack_labels = ['Normal IP', 'Attack IP']\n",
    "colors = ['green', 'red']\n",
    "\n",
    "axes[1,1].bar(attack_labels, attack_success.values, color=colors, alpha=0.7)\n",
    "axes[1,1].set_title('Success Rate: Normal vs Attack IPs', fontweight='bold')\n",
    "axes[1,1].set_ylabel('Success Rate (%)')\n",
    "\n",
    "# 6. Top Risky Countries (High failure + High attack rate)\n",
    "risk_analysis = df.groupby('Country').agg({\n",
    "    'Login Successful': ['count', 'mean'],\n",
    "    'Is Attack IP': 'mean'\n",
    "}).reset_index()\n",
    "risk_analysis.columns = ['Country', 'Total_Attempts', 'Success_Rate', 'Attack_Rate']\n",
    "risk_analysis = risk_analysis[risk_analysis['Total_Attempts'] >= 50]\n",
    "risk_analysis['Risk_Score'] = (1 - risk_analysis['Success_Rate']) + risk_analysis['Attack_Rate']\n",
    "risk_analysis = risk_analysis.sort_values('Risk_Score', ascending=False).head(8)\n",
    "\n",
    "axes[1,2].barh(risk_analysis['Country'], risk_analysis['Risk_Score'], color='orange', alpha=0.7)\n",
    "axes[1,2].set_title('Country Risk Score (Failure + Attack Rate)', fontweight='bold')\n",
    "axes[1,2].set_xlabel('Risk Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed risk analysis\n",
    "print(\"\\nüéØ DETAILED RISK ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"üö® HIGHEST RISK COUNTRIES:\")\n",
    "for _, row in risk_analysis.head(5).iterrows():\n",
    "    print(f\"  {row['Country']}: Risk Score {row['Risk_Score']:.2f}\")\n",
    "    print(f\"    Success Rate: {row['Success_Rate']:.1%}, Attack Rate: {row['Attack_Rate']:.1%}\")\n",
    "\n",
    "# Time-based risk patterns\n",
    "risky_hours = hourly_data.nlargest(3, 'Attack_Rate')\n",
    "print(f\"\\n‚è∞ HIGHEST RISK HOURS:\")\n",
    "for _, row in risky_hours.iterrows():\n",
    "    print(f\"  {row['Hour']:02d}:00 - Attack Rate: {row['Attack_Rate']:.1%}\")\n",
    "\n",
    "# Device security analysis\n",
    "print(f\"\\nüì± DEVICE SECURITY ANALYSIS:\")\n",
    "for _, row in device_success.iterrows():\n",
    "    if row['Count'] >= 10:  # Only devices with enough data\n",
    "        print(f\"  {row['Device_Type']}: Success {row['Success_Rate']:.1%}, Attack Rate {row['Attack_Rate']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c18e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ BantAI Demo Setup\n",
      "Load your RBA dataset first, then run:\n",
      "bantai_model = demo_bantai_model(df)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BantAI: Filipino-Centric Risk-Based Authentication Model\n",
    "A specialized RBA system for Philippine digital banking with explainable AI,\n",
    "multilingual support, and offline capabilities.\n",
    "\n",
    "Project: BPI DATA WAVE 2025 - \"I got reincarnated as a CS student\"\n",
    "Team: Era | Donato | Siaton | Carpio\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BantAI_RBA_Model:\n",
    "    \"\"\"\n",
    "    BantAI: Filipino-Centric Risk-Based Authentication Model\n",
    "    \n",
    "    Features:\n",
    "    - Philippine geographic context\n",
    "    - Explainable AI decisions\n",
    "    - Multilingual risk explanations\n",
    "    - Offline-capable lightweight model\n",
    "    - Zero-trust security principles\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = []\n",
    "        self.risk_rules = {}\n",
    "        self.filipino_regions = self._load_filipino_geography()\n",
    "        self.multilingual_messages = self._load_multilingual_messages()\n",
    "        \n",
    "    def _load_filipino_geography(self):\n",
    "        \"\"\"\n",
    "        Philippine geographic data for location-based risk assessment\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'metro_manila': ['Manila', 'Quezon City', 'Makati', 'Taguig', 'Pasig', 'Mandaluyong', \n",
    "                           'San Juan', 'Marikina', 'Pasay', 'Caloocan', 'Malabon', 'Navotas',\n",
    "                           'Valenzuela', 'Las Pi√±as', 'Muntinlupa', 'Para√±aque', 'Pateros'],\n",
    "            'luzon_major': ['Cebu', 'Baguio', 'Dagupan', 'Angeles', 'San Fernando', 'Cabanatuan',\n",
    "                          'Olongapo', 'Batangas', 'Lipa', 'Lucena', 'Naga', 'Legazpi'],\n",
    "            'visayas_major': ['Cebu City', 'Mandaue', 'Lapu-Lapu', 'Iloilo City', 'Bacolod',\n",
    "                            'Dumaguete', 'Tacloban', 'Ormoc', 'Tagbilaran'],\n",
    "            'mindanao_major': ['Davao City', 'Cagayan de Oro', 'Zamboanga', 'Butuan', 'Iligan',\n",
    "                             'Cotabato', 'General Santos', 'Koronadal', 'Kidapawan']\n",
    "        }\n",
    "    \n",
    "    def _load_multilingual_messages(self):\n",
    "        \"\"\"\n",
    "        Multilingual risk messages for Filipino users\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'english': {\n",
    "                'low_risk': \"Login successful. Welcome!\",\n",
    "                'medium_risk': \"Additional verification required for security.\",\n",
    "                'high_risk': \"Suspicious activity detected. Access temporarily restricted.\",\n",
    "                'otp_required': \"Please enter the OTP sent to your mobile number.\"\n",
    "            },\n",
    "            'tagalog': {\n",
    "                'low_risk': \"Matagumpay na pag-login. Maligayang pagdating!\",\n",
    "                'medium_risk': \"Kailangan ng karagdagang verification para sa seguridad.\",\n",
    "                'high_risk': \"May nakitang kakaibang aktibidad. Pansamantalang limitado ang access.\",\n",
    "                'otp_required': \"Pakisulit ang OTP na naipadala sa inyong mobile number.\"\n",
    "            },\n",
    "            'bisaya': {\n",
    "                'low_risk': \"Malampuson nga pag-login. Maayong pag-abot!\",\n",
    "                'medium_risk': \"Kinahanglan og dugang verification para sa seguridad.\",\n",
    "                'high_risk': \"Adunay suspicious nga kalihokan. Temporaryo nga limitado ang access.\",\n",
    "                'otp_required': \"Palihug ibutang ang OTP nga gipadala sa inyong mobile.\"\n",
    "            },\n",
    "            'ilocano': {\n",
    "                'low_risk': \"Naballigi ti pag-login. Naragsak nga isasangbay!\",\n",
    "                'medium_risk': \"Kasapulan ti kanayonan a verification para iti seguridad.\",\n",
    "                'high_risk': \"Adda nakita a karkarna nga aramid. Temporario a limitado ti access.\",\n",
    "                'otp_required': \"Pangngaasiyo nga ikabil ti OTP a naipatulod iti mobile number yo.\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def preprocess_filipino_rba_data(self, df):\n",
    "        \"\"\"\n",
    "        Preprocess RBA data with Filipino banking context\n",
    "        \"\"\"\n",
    "        print(\"üáµüá≠ Preprocessing data for Filipino banking context...\")\n",
    "        \n",
    "        # Create Filipino-specific features\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Geographic risk scoring for Philippines\n",
    "        df_processed['is_metro_manila'] = df_processed['City'].isin(self.filipino_regions['metro_manila'])\n",
    "        df_processed['is_major_city'] = (\n",
    "            df_processed['City'].isin(self.filipino_regions['luzon_major'] + \n",
    "                                    self.filipino_regions['visayas_major'] + \n",
    "                                    self.filipino_regions['mindanao_major'])\n",
    "        )\n",
    "        df_processed['is_foreign_access'] = df_processed['Country'] != 'PH'\n",
    "        \n",
    "        # Time-based features for Philippine context\n",
    "        df_processed['Login Timestamp'] = pd.to_datetime(df_processed['Login Timestamp'])\n",
    "        df_processed['hour'] = df_processed['Login Timestamp'].dt.hour\n",
    "        df_processed['day_of_week'] = df_processed['Login Timestamp'].dt.dayofweek\n",
    "        df_processed['is_weekend'] = df_processed['day_of_week'].isin([5, 6])\n",
    "        df_processed['is_business_hours'] = df_processed['hour'].between(8, 17)\n",
    "        df_processed['is_night_access'] = df_processed['hour'].between(22, 5)\n",
    "        \n",
    "        # Device and network features\n",
    "        df_processed['is_mobile_device'] = df_processed['Device Type'] == 'mobile'\n",
    "        df_processed['is_shared_device'] = df_processed['Device Type'].isin(['desktop', 'tablet'])\n",
    "        \n",
    "        # Create risk score based on multiple factors\n",
    "        df_processed['calculated_risk_score'] = self._calculate_filipino_risk_score(df_processed)\n",
    "        \n",
    "        # Convert to binary classification (high risk vs normal)\n",
    "        risk_threshold = 0.6  # Adjustable threshold\n",
    "        df_processed['is_high_risk'] = (df_processed['calculated_risk_score'] > risk_threshold).astype(int)\n",
    "        \n",
    "        return df_processed\n",
    "    \n",
    "    def _calculate_filipino_risk_score(self, df):\n",
    "        \"\"\"\n",
    "        Calculate risk score based on Filipino banking patterns\n",
    "        \"\"\"\n",
    "        risk_score = np.zeros(len(df))\n",
    "        \n",
    "        # Geographic risk factors\n",
    "        risk_score += df['is_foreign_access'] * 0.4  # Foreign access = high risk\n",
    "        risk_score += (~df['is_metro_manila'] & ~df['is_major_city']) * 0.2  # Rural access = medium risk\n",
    "        \n",
    "        # Time-based risk factors\n",
    "        risk_score += df['is_night_access'] * 0.3  # Night access = high risk\n",
    "        risk_score += (~df['is_business_hours'] & ~df['is_weekend']) * 0.1  # Unusual hours\n",
    "        \n",
    "        # Security indicators\n",
    "        risk_score += df['Is Attack IP'] * 0.5  # Known attack IP = very high risk\n",
    "        risk_score += (~df['Login Successful']) * 0.3  # Failed attempts = high risk\n",
    "        \n",
    "        # Device-based factors\n",
    "        unusual_device = np.random.random(len(df)) < 0.1  # 10% unusual device patterns\n",
    "        risk_score += unusual_device * 0.2\n",
    "        \n",
    "        # Network latency (high latency might indicate VPN/proxy)\n",
    "        if 'Round-Trip Time [ms]' in df.columns:\n",
    "            high_latency = df['Round-Trip Time [ms]'].fillna(0) > 1000\n",
    "            risk_score += high_latency * 0.15\n",
    "        \n",
    "        # Normalize to 0-1 range\n",
    "        return np.clip(risk_score, 0, 1)\n",
    "    \n",
    "    def create_explainable_features(self, df):\n",
    "        \"\"\"\n",
    "        Create features that can be easily explained to IT personnel\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        feature_names = []\n",
    "        \n",
    "        # Geographic features\n",
    "        features.append(df['is_foreign_access'].astype(int))\n",
    "        feature_names.append('foreign_access')\n",
    "        \n",
    "        features.append(df['is_metro_manila'].astype(int))\n",
    "        feature_names.append('metro_manila_access')\n",
    "        \n",
    "        features.append(df['is_major_city'].astype(int))\n",
    "        feature_names.append('major_city_access')\n",
    "        \n",
    "        # Time features\n",
    "        features.append(df['is_night_access'].astype(int))\n",
    "        feature_names.append('night_access')\n",
    "        \n",
    "        features.append(df['is_business_hours'].astype(int))\n",
    "        feature_names.append('business_hours')\n",
    "        \n",
    "        features.append(df['is_weekend'].astype(int))\n",
    "        feature_names.append('weekend_access')\n",
    "        \n",
    "        # Device features\n",
    "        features.append(df['is_mobile_device'].astype(int))\n",
    "        feature_names.append('mobile_device')\n",
    "        \n",
    "        features.append(df['is_shared_device'].astype(int))\n",
    "        feature_names.append('shared_device')\n",
    "        \n",
    "        # Security features\n",
    "        features.append(df['Is Attack IP'].astype(int))\n",
    "        feature_names.append('attack_ip')\n",
    "        \n",
    "        # Network features\n",
    "        if 'Round-Trip Time [ms]' in df.columns:\n",
    "            high_latency = (df['Round-Trip Time [ms]'].fillna(0) > 1000).astype(int)\n",
    "            features.append(high_latency)\n",
    "            feature_names.append('high_latency')\n",
    "        \n",
    "        # User behavior features (simplified)\n",
    "        features.append(df['hour'] / 24.0)  # Normalized hour\n",
    "        feature_names.append('login_hour_normalized')\n",
    "        \n",
    "        features.append(df['day_of_week'] / 7.0)  # Normalized day of week\n",
    "        feature_names.append('day_of_week_normalized')\n",
    "        \n",
    "        self.feature_names = feature_names\n",
    "        return np.column_stack(features)\n",
    "    \n",
    "    def train_bantai_model(self, df):\n",
    "        \"\"\"\n",
    "        Train the BantAI RBA model with explainable features\n",
    "        \"\"\"\n",
    "        print(\"ü§ñ Training BantAI RBA Model...\")\n",
    "        \n",
    "        # Preprocess data\n",
    "        df_processed = self.preprocess_filipino_rba_data(df)\n",
    "        \n",
    "        # Create explainable features\n",
    "        X = self.create_explainable_features(df_processed)\n",
    "        y = df_processed['is_high_risk']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train multiple models and select best\n",
    "        models = {\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42, max_depth=5),\n",
    "            'Logistic Regression': LogisticRegression(random_state=42)\n",
    "        }\n",
    "        \n",
    "        best_score = 0\n",
    "        best_model_name = \"\"\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='roc_auc')\n",
    "            mean_score = cv_scores.mean()\n",
    "            \n",
    "            print(f\"{name}: CV AUC = {mean_score:.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "            \n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_model_name = name\n",
    "                self.model = model\n",
    "        \n",
    "        # Train best model\n",
    "        print(f\"\\nüèÜ Best model: {best_model_name} (AUC: {best_score:.4f})\")\n",
    "        self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        print(f\"Test AUC: {auc_score:.4f}\")\n",
    "        \n",
    "        # Generate feature importance for explainability\n",
    "        if hasattr(self.model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': self.model.feature_importances_\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\nüîç Feature Importance (Top 5):\")\n",
    "            for _, row in importance_df.head(5).iterrows():\n",
    "                print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "        \n",
    "        # Store for explainability\n",
    "        self.X_test = X_test_scaled\n",
    "        self.y_test = y_test\n",
    "        self.y_pred_proba = y_pred_proba\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def explain_risk_decision(self, features, risk_score, language='english'):\n",
    "        \"\"\"\n",
    "        Generate explainable risk assessment in multiple Filipino languages\n",
    "        \"\"\"\n",
    "        explanations = []\n",
    "        risk_factors = []\n",
    "        \n",
    "        # Analyze each feature\n",
    "        feature_dict = dict(zip(self.feature_names, features))\n",
    "        \n",
    "        # Geographic factors\n",
    "        if feature_dict.get('foreign_access', 0) == 1:\n",
    "            risk_factors.append(\"Foreign country access\")\n",
    "            explanations.append(\"Login attempt from outside Philippines\")\n",
    "        \n",
    "        if feature_dict.get('metro_manila_access', 0) == 0 and feature_dict.get('major_city_access', 0) == 0:\n",
    "            risk_factors.append(\"Rural area access\")\n",
    "            explanations.append(\"Login from less common location\")\n",
    "        \n",
    "        # Time factors\n",
    "        if feature_dict.get('night_access', 0) == 1:\n",
    "            risk_factors.append(\"Night time access\")\n",
    "            explanations.append(\"Login attempt during unusual hours (10PM-5AM)\")\n",
    "        \n",
    "        if feature_dict.get('business_hours', 0) == 0 and feature_dict.get('weekend_access', 0) == 0:\n",
    "            risk_factors.append(\"Off-hours access\")\n",
    "            explanations.append(\"Login outside business hours on weekday\")\n",
    "        \n",
    "        # Security factors\n",
    "        if feature_dict.get('attack_ip', 0) == 1:\n",
    "            risk_factors.append(\"Known attack IP\")\n",
    "            explanations.append(\"IP address flagged for suspicious activity\")\n",
    "        \n",
    "        # Device factors\n",
    "        if feature_dict.get('shared_device', 0) == 1:\n",
    "            risk_factors.append(\"Shared device\")\n",
    "            explanations.append(\"Desktop/tablet device (potentially shared)\")\n",
    "        \n",
    "        if feature_dict.get('high_latency', 0) == 1:\n",
    "            risk_factors.append(\"High network latency\")\n",
    "            explanations.append(\"Unusual network response time (possible VPN/proxy)\")\n",
    "        \n",
    "        # Determine risk level and message\n",
    "        if risk_score < 0.3:\n",
    "            risk_level = \"LOW\"\n",
    "            message_key = 'low_risk'\n",
    "        elif risk_score < 0.7:\n",
    "            risk_level = \"MEDIUM\"\n",
    "            message_key = 'medium_risk'\n",
    "        else:\n",
    "            risk_level = \"HIGH\"\n",
    "            message_key = 'high_risk'\n",
    "        \n",
    "        # Get localized message\n",
    "        localized_message = self.multilingual_messages[language][message_key]\n",
    "        \n",
    "        return {\n",
    "            'risk_score': risk_score,\n",
    "            'risk_level': risk_level,\n",
    "            'risk_factors': risk_factors,\n",
    "            'explanations': explanations,\n",
    "            'localized_message': localized_message,\n",
    "            'requires_otp': risk_score > 0.5,\n",
    "            'action': self._determine_action(risk_score)\n",
    "        }\n",
    "    \n",
    "    def _determine_action(self, risk_score):\n",
    "        \"\"\"\n",
    "        Determine authentication action based on risk score\n",
    "        Following zero-trust principles with degraded access\n",
    "        \"\"\"\n",
    "        if risk_score < 0.3:\n",
    "            return {\n",
    "                'decision': 'ALLOW',\n",
    "                'access_level': 'FULL',\n",
    "                'additional_auth': False,\n",
    "                'restrictions': []\n",
    "            }\n",
    "        elif risk_score < 0.5:\n",
    "            return {\n",
    "                'decision': 'ALLOW',\n",
    "                'access_level': 'FULL',\n",
    "                'additional_auth': True,\n",
    "                'auth_method': 'SMS_OTP',\n",
    "                'restrictions': []\n",
    "            }\n",
    "        elif risk_score < 0.8:\n",
    "            return {\n",
    "                'decision': 'ALLOW_DEGRADED',\n",
    "                'access_level': 'LIMITED',\n",
    "                'additional_auth': True,\n",
    "                'auth_method': 'SMS_OTP',\n",
    "                'restrictions': ['no_transfers', 'no_account_changes', 'view_only']\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'decision': 'BLOCK',\n",
    "                'access_level': 'NONE',\n",
    "                'additional_auth': False,\n",
    "                'restrictions': ['complete_block'],\n",
    "                'manual_review': True\n",
    "            }\n",
    "    \n",
    "    def predict_risk(self, login_data, language='english'):\n",
    "        \"\"\"\n",
    "        Predict risk for a new login attempt\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained yet. Call train_bantai_model() first.\")\n",
    "        \n",
    "        # Preprocess single login attempt\n",
    "        df_single = pd.DataFrame([login_data])\n",
    "        df_processed = self.preprocess_filipino_rba_data(df_single)\n",
    "        \n",
    "        # Create features\n",
    "        features = self.create_explainable_features(df_processed)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        # Predict\n",
    "        risk_score = self.model.predict_proba(features_scaled)[0, 1]\n",
    "        \n",
    "        # Generate explanation\n",
    "        explanation = self.explain_risk_decision(features[0], risk_score, language)\n",
    "        \n",
    "        return explanation\n",
    "    \n",
    "    def generate_security_dashboard_data(self, df):\n",
    "        \"\"\"\n",
    "        Generate data for the security team dashboard\n",
    "        \"\"\"\n",
    "        df_processed = self.preprocess_filipino_rba_data(df)\n",
    "        \n",
    "        dashboard_data = {\n",
    "            'total_attempts': len(df_processed),\n",
    "            'success_rate': df_processed['Login Successful'].mean(),\n",
    "            'high_risk_attempts': df_processed['is_high_risk'].sum(),\n",
    "            'foreign_attempts': df_processed['is_foreign_access'].sum(),\n",
    "            'night_attempts': df_processed['is_night_access'].sum(),\n",
    "            'attack_ip_attempts': df_processed['Is Attack IP'].sum(),\n",
    "            \n",
    "            # Geographic breakdown\n",
    "            'metro_manila_success': df_processed[df_processed['is_metro_manila']]['Login Successful'].mean(),\n",
    "            'rural_success': df_processed[~df_processed['is_metro_manila'] & ~df_processed['is_major_city']]['Login Successful'].mean(),\n",
    "            \n",
    "            # Time patterns\n",
    "            'business_hours_success': df_processed[df_processed['is_business_hours']]['Login Successful'].mean(),\n",
    "            'night_success': df_processed[df_processed['is_night_access']]['Login Successful'].mean(),\n",
    "            \n",
    "            # Device patterns\n",
    "            'mobile_success': df_processed[df_processed['is_mobile_device']]['Login Successful'].mean(),\n",
    "            'desktop_success': df_processed[df_processed['is_shared_device']]['Login Successful'].mean(),\n",
    "        }\n",
    "        \n",
    "        return dashboard_data\n",
    "\n",
    "def demo_bantai_model(df):\n",
    "    \"\"\"\n",
    "    Demonstration of BantAI RBA model with Filipino context\n",
    "    \"\"\"\n",
    "    print(\"üáµüá≠ BANTAI: FILIPINO-CENTRIC RBA MODEL DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize BantAI\n",
    "    bantai = BantAI_RBA_Model()\n",
    "    \n",
    "    # Train model\n",
    "    model = bantai.train_bantai_model(df)\n",
    "    \n",
    "    # Generate dashboard data\n",
    "    dashboard = bantai.generate_security_dashboard_data(df)\n",
    "    \n",
    "    print(f\"\\nüìä SECURITY DASHBOARD SUMMARY\")\n",
    "    print(f\"Total Login Attempts: {dashboard['total_attempts']:,}\")\n",
    "    print(f\"Overall Success Rate: {dashboard['success_rate']:.1%}\")\n",
    "    print(f\"High Risk Attempts: {dashboard['high_risk_attempts']:,}\")\n",
    "    print(f\"Foreign Access Attempts: {dashboard['foreign_attempts']:,}\")\n",
    "    print(f\"Night Time Attempts: {dashboard['night_attempts']:,}\")\n",
    "    \n",
    "    print(f\"\\nüèôÔ∏è LOCATION-BASED INSIGHTS\")\n",
    "    print(f\"Metro Manila Success Rate: {dashboard['metro_manila_success']:.1%}\")\n",
    "    print(f\"Rural Areas Success Rate: {dashboard['rural_success']:.1%}\")\n",
    "    \n",
    "    # Test multilingual explanations\n",
    "    print(f\"\\nüó£Ô∏è MULTILINGUAL RISK EXPLANATIONS\")\n",
    "    \n",
    "    # Create test scenarios\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            'scenario': 'Normal Manila login',\n",
    "            'data': {\n",
    "                'City': 'Manila',\n",
    "                'Country': 'PH',\n",
    "                'Login Timestamp': '2024-01-15 14:30:00',\n",
    "                'Device Type': 'mobile',\n",
    "                'Is Attack IP': False,\n",
    "                'Login Successful': True,\n",
    "                'Round-Trip Time [ms]': 45\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'scenario': 'Suspicious foreign login',\n",
    "            'data': {\n",
    "                'City': 'Beijing',\n",
    "                'Country': 'CN',\n",
    "                'Login Timestamp': '2024-01-15 02:30:00',\n",
    "                'Device Type': 'desktop',\n",
    "                'Is Attack IP': True,\n",
    "                'Login Successful': False,\n",
    "                'Round-Trip Time [ms]': 1500\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    languages = ['english', 'tagalog', 'bisaya']\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\nüîç {scenario['scenario'].upper()}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for lang in languages:\n",
    "            result = bantai.predict_risk(scenario['data'], language=lang)\n",
    "            print(f\"{lang.capitalize()}: {result['localized_message']}\")\n",
    "            print(f\"  Risk Score: {result['risk_score']:.2f} ({result['risk_level']})\")\n",
    "            print(f\"  Action: {result['action']['decision']}\")\n",
    "            if result['risk_factors']:\n",
    "                print(f\"  Risk Factors: {', '.join(result['risk_factors'])}\")\n",
    "    \n",
    "    return bantai\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ BantAI Demo Setup\")\n",
    "    print(\"Load your RBA dataset first, then run:\")\n",
    "    print(\"bantai_model = demo_bantai_model(df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93845723",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'demo_bantai_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m bantai_model = \u001b[43mdemo_bantai_model\u001b[49m(df)\n",
      "\u001b[31mNameError\u001b[39m: name 'demo_bantai_model' is not defined"
     ]
    }
   ],
   "source": [
    "bantai_model = demo_bantai_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c35dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['index', 'Login Timestamp', 'User ID', 'Round-Trip Time [ms]',\n",
      "       'IP Address', 'Country', 'Region', 'City', 'ASN', 'User Agent String',\n",
      "       'Browser Name and Version', 'OS Name and Version', 'Device Type',\n",
      "       'Login Successful', 'Is Attack IP', 'Is Account Takeover'],\n",
      "      dtype='object')\n",
      "   index          Login Timestamp              User ID  Round-Trip Time [ms]  \\\n",
      "0      0  2020-02-03 12:43:30.772 -4324475583306591935                   NaN   \n",
      "1      1  2020-02-03 12:43:43.549 -4324475583306591935                   NaN   \n",
      "2      2  2020-02-03 12:43:55.873 -3284137479262433373                   NaN   \n",
      "3      3  2020-02-03 12:43:56.180 -4324475583306591935                   NaN   \n",
      "4      4  2020-02-03 12:43:59.396 -4618854071942621186                   NaN   \n",
      "\n",
      "      IP Address Country    Region       City     ASN  \\\n",
      "0    10.0.65.171      NO         -          -   29695   \n",
      "1   194.87.207.6      AU         -          -   60117   \n",
      "2  81.167.144.58      NO  Vestland  Urangsvag   29695   \n",
      "3  170.39.78.152      US         -          -  393398   \n",
      "4      10.0.0.47      US  Virginia    Ashburn  398986   \n",
      "\n",
      "                                   User Agent String  \\\n",
      "0  Mozilla/5.0  (iPhone; CPU iPhone OS 13_4 like ...   \n",
      "1  Mozilla/5.0  (Linux; Android 4.1; Galaxy Nexus...   \n",
      "2  Mozilla/5.0  (iPad; CPU OS 7_1 like Mac OS X) ...   \n",
      "3  Mozilla/5.0  (Linux; Android 4.1; Galaxy Nexus...   \n",
      "4  Mozilla/5.0  (Linux; U; Android 2.2) Build/NMA...   \n",
      "\n",
      "          Browser Name and Version OS Name and Version Device Type  \\\n",
      "0              Firefox 20.0.0.1618            iOS 13.4      mobile   \n",
      "1          Chrome Mobile 46.0.2490         Android 4.1      mobile   \n",
      "2               Android 2.3.3.2672             iOS 7.1      mobile   \n",
      "3  Chrome Mobile WebView 85.0.4183         Android 4.1      mobile   \n",
      "4  Chrome Mobile WebView 85.0.4183         Android 2.2      mobile   \n",
      "\n",
      "   Login Successful  Is Attack IP  Is Account Takeover  \n",
      "0             False         False                False  \n",
      "1             False         False                False  \n",
      "2              True         False                False  \n",
      "3             False         False                False  \n",
      "4             False          True                False  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(r\"C:\\Users\\Brando\\Desktop\\School\\Project\\BantAI_Datawave\\rba-dataset.csv\")\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "187c20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from datetime import datetime\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.distance import geodesic\n",
    "import time\n",
    "\n",
    "\n",
    "class BantAI_TravelAware:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        # initialize geolocator once\n",
    "        self.geolocator = Nominatim(user_agent=\"bantai_ai\")\n",
    "        # cache to avoid repeated API calls\n",
    "        self.geo_cache = {}\n",
    "\n",
    "    def get_coordinates(self, city, country):\n",
    "        \"\"\"Convert city + country into latitude/longitude (with caching)\"\"\"\n",
    "        if pd.isna(city) or pd.isna(country):\n",
    "            return None\n",
    "\n",
    "        key = f\"{city},{country}\"\n",
    "        if key in self.geo_cache:\n",
    "            return self.geo_cache[key]\n",
    "\n",
    "        try:\n",
    "            location = self.geolocator.geocode(key, timeout=10)\n",
    "            if location:\n",
    "                coords = (location.latitude, location.longitude)\n",
    "                self.geo_cache[key] = coords\n",
    "                time.sleep(1)  # avoid rate limit\n",
    "                return coords\n",
    "        except Exception as e:\n",
    "            print(f\"Geocoding error for {key}: {e}\")\n",
    "\n",
    "        self.geo_cache[key] = None\n",
    "        return None\n",
    "\n",
    "    def compute_distance(self, last_city, last_country, curr_city, curr_country):\n",
    "        coords1 = self.get_coordinates(last_city, last_country)\n",
    "        coords2 = self.get_coordinates(curr_city, curr_country)\n",
    "        if coords1 and coords2:\n",
    "            return geodesic(coords1, coords2).kilometers\n",
    "        return 0  # fallback if geocoding fails\n",
    "\n",
    "    def get_zone_risk(self, city):\n",
    "        \"\"\"Assign a zone risk score based on location (extendable)\"\"\"\n",
    "        risk_zones = {\"Dubai\": 3, \"Singapore\": 2, \"Manila\": 1}\n",
    "        return risk_zones.get(city, 1)\n",
    "\n",
    "    def compute_features(self, last_login, current_login):\n",
    "        distance_km = self.compute_distance(\n",
    "            last_login[\"city\"], last_login[\"country\"],\n",
    "            current_login[\"city\"], current_login[\"country\"]\n",
    "        )\n",
    "\n",
    "        time_gap = (current_login[\"time\"] - last_login[\"time\"]).total_seconds() / 3600.0\n",
    "        travel_speed = distance_km / time_gap if time_gap > 0 else 0\n",
    "        zone_risk_score = self.get_zone_risk(current_login[\"city\"])\n",
    "\n",
    "        return {\n",
    "            \"distance_km\": distance_km,\n",
    "            \"time_gap_hours\": time_gap,\n",
    "            \"travel_speed\": travel_speed,\n",
    "            \"zone_risk_score\": zone_risk_score,\n",
    "        }\n",
    "\n",
    "    def prepare_training_data(self, df):\n",
    "        \"\"\" Convert login history DataFrame into feature matrix and labels \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(1, len(df)):\n",
    "            last_login = df.iloc[i-1].to_dict()\n",
    "            current_login = df.iloc[i].to_dict()\n",
    "            features = self.compute_features(last_login, current_login)\n",
    "            X.append(list(features.values()))\n",
    "            y.append(current_login[\"label\"])  # 0 = legit, 1 = suspicious\n",
    "        return pd.DataFrame(X, columns=[\"distance_km\",\"time_gap_hours\",\"travel_speed\",\"zone_risk_score\"]), y\n",
    "\n",
    "    def train_model_from_csv(self, csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Map dataset columns into expected names\n",
    "        df = df.rename(columns={\n",
    "            \"Login Timestamp\": \"time\",\n",
    "            \"Country\": \"country\",\n",
    "            \"City\": \"city\",\n",
    "            \"Is Attack IP\": \"label\"   # <-- OR use \"Is Account Takeover\"\n",
    "        })\n",
    "\n",
    "        # Ensure time is datetime\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"time\", \"city\", \"country\", \"label\"])\n",
    "        df = df.sort_values(\"time\")\n",
    "\n",
    "        X, y = self.prepare_training_data(df)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"Model Performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        self.model = clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6a5932",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m bantay = BantAI_TravelAware()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbantay\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model_from_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBrando\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDesktop\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSchool\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mProject\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mBantAI_Datawave\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mrba-dataset.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mBantAI_TravelAware.train_model_from_csv\u001b[39m\u001b[34m(self, csv_path)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_model_from_csv\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_path):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# Map dataset columns into expected names\u001b[39;00m\n\u001b[32m     85\u001b[39m     df = df.rename(columns={\n\u001b[32m     86\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLogin Timestamp\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCountry\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCity\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mcity\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     89\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIs Attack IP\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# <-- OR use \"Is Account Takeover\"\u001b[39;00m\n\u001b[32m     90\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brando\\miniconda3\\envs\\BantAI\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brando\\miniconda3\\envs\\BantAI\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brando\\miniconda3\\envs\\BantAI\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Brando\\miniconda3\\envs\\BantAI\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2053\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:334\u001b[39m, in \u001b[36mgetstate\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "bantay = BantAI_TravelAware()\n",
    "bantay.train_model_from_csv(r\"C:\\Users\\Brando\\Desktop\\School\\Project\\BantAI_Datawave\\rba-dataset.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b46059",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_login = {\"city\": \"Manila\", \"country\": \"PH\", \"time\": datetime(2025, 9, 10, 8, 0)}\n",
    "current_login = {\"city\": \"Dubai\", \"country\": \"AE\", \"time\": datetime(2025, 9, 11, 12, 0)}\n",
    "\n",
    "prediction = bantai.predict_login(last_login, current_login)\n",
    "print(\"Prediction:\", \"Suspicious\" if prediction == 1 else \"Legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: predict on new login\n",
    "last_login = {\"location\": \"Manila\", \"time\": datetime(2025, 9, 11, 9, 0)}\n",
    "new_login = {\"location\": \"Dubai\", \"time\": datetime(2025, 9, 11, 12, 0)}\n",
    "prediction = bantay.predict_login(last_login, new_login)\n",
    "\n",
    "print(\"Suspicious\" if prediction == 1 else \"Legit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49f48d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class BantAI_TravelAware:\n",
    "    def __init__(self):\n",
    "        self.login_history = []\n",
    "        self.baseline_profile = {}\n",
    "        self.model = None  # ML model will be stored here\n",
    "\n",
    "    def compute_features(self, last_login, current_login):\n",
    "        distance_km = self.compute_distance(last_login[\"location\"], current_login[\"location\"])\n",
    "        time_gap = (current_login[\"time\"] - last_login[\"time\"]).total_seconds() / 3600.0\n",
    "        travel_speed = distance_km / time_gap if time_gap > 0 else 0\n",
    "        zone_risk_score = self.get_zone_risk(current_login[\"location\"])\n",
    "\n",
    "        return {\n",
    "            \"distance_km\": distance_km,\n",
    "            \"time_gap_hours\": time_gap,\n",
    "            \"travel_speed\": travel_speed,\n",
    "            \"zone_risk_score\": zone_risk_score,\n",
    "        }\n",
    "\n",
    "    def prepare_training_data(self, labeled_logins):\n",
    "        \"\"\" Convert login history into feature matrix and labels \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(1, len(labeled_logins)):\n",
    "            last_login = labeled_logins[i-1]\n",
    "            current_login = labeled_logins[i]\n",
    "            features = self.compute_features(last_login, current_login)\n",
    "            X.append(list(features.values()))\n",
    "            y.append(current_login[\"label\"])  # 0 = legit, 1 = suspicious\n",
    "        return pd.DataFrame(X, columns=[\"distance_km\",\"time_gap_hours\",\"travel_speed\",\"zone_risk_score\"]), y\n",
    "\n",
    "    def train_model(self, labeled_logins):\n",
    "        X, y = self.prepare_training_data(labeled_logins)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"Model Performance:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "        self.model = clf\n",
    "\n",
    "    def predict_login(self, last_login, current_login):\n",
    "        \"\"\" Predict suspicious vs legit login \"\"\"\n",
    "        if not self.model:\n",
    "            raise Exception(\"Model not trained yet.\")\n",
    "\n",
    "        features = self.compute_features(last_login, current_login)\n",
    "        X_new = pd.DataFrame([features])\n",
    "        return self.model.predict(X_new)[0]  # 0 = legit, 1 = suspicious\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "665eb7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ´ BANTAI TRAVEL-AWARE RBA DEMO\n",
      "==================================================\n",
      "\n",
      "üìä ESTABLISHING USER BASELINE...\n",
      "‚úÖ Baseline established for User juan_dela_cruz_123\n",
      "   Home locations: ['Manila', 'Makati', 'Quezon City']\n",
      "   Countries visited: ['PH']\n",
      "   Common devices: ['desktop', 'mobile']\n",
      "\n",
      "üîç ANALYZING TRAVEL SCENARIOS...\n",
      "==================================================\n",
      "\n",
      "‚úàÔ∏è Legitimate OFW Travel to Dubai\n",
      "----------------------------------------\n",
      "üìç Location: Dubai, AE\n",
      "üéØ Risk Score: 0.13 (LOW)\n",
      "‚ö° Action: ALLOW\n",
      "‚úÖ Travel Plausible: True\n",
      "üîÑ Behavior Consistent: True\n",
      "üí° Recommendation: ALLOW with SMS OTP: Possible legitimate travel, verify with additional authentication.\n",
      "\n",
      "üîç Detailed Analysis:\n",
      "   ‚úÖ Travel is plausible (Sufficient time for travel (251.2h vs 11.7h required))\n",
      "   üîç Behavior consistency: 80%\n",
      "   üìç Location: Major OFW employment hubs in Middle East\n",
      "\n",
      "üö® Impossible Travel Attack\n",
      "----------------------------------------\n",
      "üìç Location: Moscow, RU\n",
      "üéØ Risk Score: 0.48 (MEDIUM)\n",
      "‚ö° Action: ALLOW_WITH_OTP\n",
      "‚úÖ Travel Plausible: True\n",
      "üîÑ Behavior Consistent: False\n",
      "üí° Recommendation: ALLOW with SMS OTP: Possible legitimate travel, verify with additional authentication.\n",
      "\n",
      "üîç Detailed Analysis:\n",
      "   ‚úÖ Travel is plausible (Sufficient time for travel (252.2h vs 13.2h required))\n",
      "   üîç Behavior consistency: 60%\n",
      "   üìç Location: Known cybercrime and state-sponsored threat locations\n",
      "   ‚ö†Ô∏è Known attack IP\n",
      "   ‚ö†Ô∏è High network latency\n",
      "   ‚ö†Ô∏è Failed login attempt\n",
      "\n",
      "üõÇ Business Trip to Singapore\n",
      "----------------------------------------\n",
      "üìç Location: Singapore, SG\n",
      "üéØ Risk Score: 0.14 (LOW)\n",
      "‚ö° Action: ALLOW\n",
      "‚úÖ Travel Plausible: True\n",
      "üîÑ Behavior Consistent: True\n",
      "üí° Recommendation: ALLOW with SMS OTP: Possible legitimate travel, verify with additional authentication.\n",
      "\n",
      "üîç Detailed Analysis:\n",
      "   ‚úÖ Travel is plausible (Sufficient time for travel (361.7h vs 6.7h required))\n",
      "   üîç Behavior consistency: 80%\n",
      "   üìç Location: Regional and global business centers\n",
      "\n",
      "üë®‚Äçüíº Return to Philippines\n",
      "----------------------------------------\n",
      "üìç Location: Manila, PH\n",
      "üéØ Risk Score: 0.03 (LOW)\n",
      "‚ö° Action: ALLOW\n",
      "‚úÖ Travel Plausible: True\n",
      "üîÑ Behavior Consistent: True\n",
      "üí° Recommendation: ALLOW: Legitimate travel with consistent behavior.\n",
      "\n",
      "üîç Detailed Analysis:\n",
      "   ‚úÖ Travel is plausible (Same location or local area)\n",
      "   üîç Behavior consistency: 90%\n",
      "   üìç Location: Philippine domestic locations\n",
      "\n",
      "üí° Usage for Custom Scenarios:\n",
      "# 1. Establish user baseline:\n",
      "baseline = travel_system.analyze_user_baseline(user_login_history)\n",
      "\n",
      "# 2. Analyze new login:\n",
      "risk_analysis = travel_system.calculate_travel_aware_risk(user_id, new_login)\n",
      "explanation = travel_system.generate_travel_aware_explanation(risk_analysis)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BantAI Travel-Aware Risk-Based Authentication System\n",
    "Separate implementation for intelligent travel vs threat detection\n",
    "\n",
    "Distinguishes between:\n",
    "- Legitimate Filipino travelers (OFWs, tourists, business)\n",
    "- Account compromise/cyber attacks\n",
    "\n",
    "Features:\n",
    "- Travel plausibility analysis\n",
    "- Behavioral consistency scoring\n",
    "- Impossible travel detection\n",
    "- OFW-friendly risk assessment\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from geopy.distance import geodesic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class BantAI_TravelAware:\n",
    "    \"\"\"\n",
    "    Travel-Aware Risk-Based Authentication for Filipino Banking\n",
    "    \n",
    "    Smart enough to distinguish between:\n",
    "    - Juan traveling to Dubai for work (legitimate)\n",
    "    - Hacker accessing from Moscow (threat)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_profiles = {}  # Store user behavioral baselines\n",
    "        self.location_coordinates = self._load_location_data()\n",
    "        self.travel_risk_zones = self._define_travel_zones()\n",
    "        self.max_travel_speed_kmh = 900  # Commercial aircraft speed\n",
    "        \n",
    "    def _load_location_data(self):\n",
    "        \"\"\"\n",
    "        Comprehensive geographic coordinates for travel distance calculations\n",
    "        Covering major Philippine cities and global destinations\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # PHILIPPINES - Major Cities and Regions\n",
    "            'Manila': (14.5995, 120.9842),\n",
    "            'Quezon City': (14.6760, 121.0437),\n",
    "            'Makati': (14.5547, 121.0244),\n",
    "            'Taguig': (14.5176, 121.0509),\n",
    "            'Pasig': (14.5764, 121.0851),\n",
    "            'Mandaluyong': (14.5794, 121.0359),\n",
    "            'Marikina': (14.6507, 121.1029),\n",
    "            'Pasay': (14.5378, 120.9896),\n",
    "            'Para√±aque': (14.4793, 121.0198),\n",
    "            'Las Pi√±as': (14.4304, 120.9820),\n",
    "            'Muntinlupa': (14.4037, 121.0270),\n",
    "            'Caloocan': (14.6488, 120.9658),\n",
    "            'Valenzuela': (14.6958, 120.9830),\n",
    "            'Malabon': (14.6570, 120.9658),\n",
    "            'Navotas': (14.6691, 120.9618),\n",
    "            \n",
    "            # Luzon Cities\n",
    "            'Baguio': (16.4023, 120.5960),\n",
    "            'Angeles': (15.1450, 120.5864),\n",
    "            'San Fernando': (15.0332, 120.6833),\n",
    "            'Dagupan': (16.0433, 120.3433),\n",
    "            'Cabanatuan': (15.4891, 120.9627),\n",
    "            'Olongapo': (14.8294, 120.2824),\n",
    "            'Batangas': (13.7565, 121.0583),\n",
    "            'Lipa': (13.9411, 121.1634),\n",
    "            'Lucena': (13.9373, 121.617),\n",
    "            'Naga': (13.6218, 123.1948),\n",
    "            'Legazpi': (13.1391, 123.7437),\n",
    "            'Iloilo City': (10.7202, 122.5621),\n",
    "            'Vigan': (17.5748, 120.3871),\n",
    "            'Tuguegarao': (17.6132, 121.7270),\n",
    "            'Laoag': (18.1967, 120.5929),\n",
    "            \n",
    "            # Visayas Cities\n",
    "            'Cebu City': (10.3157, 123.8854),\n",
    "            'Mandaue': (10.3237, 123.9227),\n",
    "            'Lapu-Lapu': (10.3103, 123.9494),\n",
    "            'Bacolod': (10.6740, 122.9540),\n",
    "            'Dumaguete': (9.3067, 123.3065),\n",
    "            'Tacloban': (11.2447, 125.0048),\n",
    "            'Ormoc': (11.0059, 124.6074),\n",
    "            'Tagbilaran': (9.6496, 123.8543),\n",
    "            'Roxas': (11.5854, 122.7511),\n",
    "            'Kalibo': (11.7040, 122.3690),\n",
    "            \n",
    "            # Mindanao Cities\n",
    "            'Davao City': (7.1907, 125.4553),\n",
    "            'Cagayan de Oro': (8.4542, 124.6319),\n",
    "            'Zamboanga': (6.9214, 122.0790),\n",
    "            'Butuan': (8.9470, 125.5406),\n",
    "            'Iligan': (8.2280, 124.2452),\n",
    "            'Cotabato': (7.2231, 124.2467),\n",
    "            'General Santos': (6.1164, 125.1716),\n",
    "            'Koronadal': (6.5000, 124.8500),\n",
    "            'Kidapawan': (7.0103, 125.0890),\n",
    "            'Dipolog': (8.5958, 123.3417),\n",
    "            'Pagadian': (7.8272, 123.4433),\n",
    "            'Marawi': (8.0021, 124.2979),\n",
    "            \n",
    "            # MIDDLE EAST - Major OFW Destinations\n",
    "            'Dubai': (25.2048, 55.2708),\n",
    "            'Abu Dhabi': (24.4539, 54.3773),\n",
    "            'Sharjah': (25.3573, 55.4033),\n",
    "            'Ajman': (25.4052, 55.5136),\n",
    "            'Al Ain': (24.2075, 55.7647),\n",
    "            'Riyadh': (24.7136, 46.6753),\n",
    "            'Jeddah': (21.4858, 39.1925),\n",
    "            'Dammam': (26.4207, 50.0888),\n",
    "            'Mecca': (21.3891, 39.8579),\n",
    "            'Medina': (24.5247, 39.5692),\n",
    "            'Doha': (25.2854, 51.5310),\n",
    "            'Kuwait City': (29.3117, 47.4818),\n",
    "            'Manama': (26.2285, 50.5860),\n",
    "            'Muscat': (23.5880, 58.3829),\n",
    "            'Amman': (31.9539, 35.9106),\n",
    "            'Beirut': (33.8938, 35.5018),\n",
    "            'Baghdad': (33.3152, 44.3661),\n",
    "            'Tehran': (35.6892, 51.3890),\n",
    "            'Isfahan': (32.6546, 51.6680),\n",
    "            'Mashhad': (36.2605, 59.6168),\n",
    "            \n",
    "            # ASIA PACIFIC - Business and Tourism Hubs\n",
    "            'Singapore': (1.3521, 103.8198),\n",
    "            'Hong Kong': (22.3193, 114.1694),\n",
    "            'Macau': (22.1987, 113.5439),\n",
    "            'Tokyo': (35.6762, 139.6503),\n",
    "            'Osaka': (34.6937, 135.5023),\n",
    "            'Nagoya': (35.1815, 136.9066),\n",
    "            'Kyoto': (35.0116, 135.7681),\n",
    "            'Yokohama': (35.4437, 139.6380),\n",
    "            'Seoul': (37.5665, 126.9780),\n",
    "            'Busan': (35.1796, 129.0756),\n",
    "            'Incheon': (37.4563, 126.7052),\n",
    "            'Bangkok': (13.7563, 100.5018),\n",
    "            'Phuket': (7.8804, 98.3923),\n",
    "            'Pattaya': (12.9236, 100.8825),\n",
    "            'Kuala Lumpur': (3.1390, 101.6869),\n",
    "            'Johor Bahru': (1.4927, 103.7414),\n",
    "            'Penang': (5.4164, 100.3327),\n",
    "            'Jakarta': (6.2088, 106.8456),\n",
    "            'Bali': (8.3405, 115.0920),\n",
    "            'Surabaya': (7.2575, 112.7521),\n",
    "            'Ho Chi Minh City': (10.8231, 106.6297),\n",
    "            'Hanoi': (21.0285, 105.8542),\n",
    "            'Da Nang': (16.0544, 108.2022),\n",
    "            'Phnom Penh': (11.5564, 104.9282),\n",
    "            'Vientiane': (17.9757, 102.6331),\n",
    "            'Yangon': (16.8661, 96.1951),\n",
    "            'Colombo': (6.9271, 79.8612),\n",
    "            'Dhaka': (23.8103, 90.4125),\n",
    "            'Kathmandu': (27.7172, 85.3240),\n",
    "            \n",
    "            # NORTH AMERICA - Filipino Diaspora Communities\n",
    "            'Los Angeles': (34.0522, -118.2437),\n",
    "            'San Francisco': (37.7749, -122.4194),\n",
    "            'San Diego': (32.7157, -117.1611),\n",
    "            'Las Vegas': (36.1699, -115.1398),\n",
    "            'Phoenix': (33.4484, -112.0740),\n",
    "            'Seattle': (47.6062, -122.3321),\n",
    "            'Portland': (45.5152, -122.6784),\n",
    "            'Sacramento': (38.5816, -121.4944),\n",
    "            'Fresno': (36.7378, -119.7871),\n",
    "            'San Jose': (37.3382, -121.8863),\n",
    "            'New York': (40.7128, -74.0060),\n",
    "            'Jersey City': (40.7178, -74.0431),\n",
    "            'Philadelphia': (39.9526, -75.1652),\n",
    "            'Washington DC': (38.9072, -77.0369),\n",
    "            'Boston': (42.3601, -71.0589),\n",
    "            'Chicago': (41.8781, -87.6298),\n",
    "            'Detroit': (42.3314, -83.0458),\n",
    "            'Miami': (25.7617, -80.1918),\n",
    "            'Orlando': (28.5383, -81.3792),\n",
    "            'Tampa': (27.9506, -82.4572),\n",
    "            'Houston': (29.7604, -95.3698),\n",
    "            'Dallas': (32.7767, -96.7970),\n",
    "            'Austin': (30.2672, -97.7431),\n",
    "            'San Antonio': (29.4241, -98.4936),\n",
    "            'Denver': (39.7392, -104.9903),\n",
    "            'Atlanta': (33.7490, -84.3880),\n",
    "            'Honolulu': (21.3099, -157.8581),\n",
    "            'Anchorage': (61.2181, -149.9003),\n",
    "            \n",
    "            # CANADA\n",
    "            'Toronto': (43.6532, -79.3832),\n",
    "            'Vancouver': (49.2827, -123.1207),\n",
    "            'Montreal': (45.5017, -73.5673),\n",
    "            'Calgary': (51.0447, -114.0719),\n",
    "            'Edmonton': (53.5461, -113.4938),\n",
    "            'Ottawa': (45.4215, -75.6972),\n",
    "            'Winnipeg': (49.8951, -97.1384),\n",
    "            'Quebec City': (46.8139, -71.2080),\n",
    "            'Hamilton': (43.2557, -79.8711),\n",
    "            \n",
    "            # EUROPE - Tourism and Business\n",
    "            'London': (51.5074, -0.1278),\n",
    "            'Manchester': (53.4808, -2.2426),\n",
    "            'Birmingham': (52.4862, -1.8904),\n",
    "            'Edinburgh': (55.9533, -3.1883),\n",
    "            'Glasgow': (55.8642, -4.2518),\n",
    "            'Dublin': (53.3498, -6.2603),\n",
    "            'Paris': (48.8566, 2.3522),\n",
    "            'Lyon': (45.7640, 4.8357),\n",
    "            'Marseille': (43.2965, 5.3698),\n",
    "            'Rome': (41.9028, 12.4964),\n",
    "            'Milan': (45.4642, 9.1900),\n",
    "            'Naples': (40.8518, 14.2681),\n",
    "            'Venice': (45.4408, 12.3155),\n",
    "            'Madrid': (40.4168, -3.7038),\n",
    "            'Barcelona': (41.3851, 2.1734),\n",
    "            'Berlin': (52.5200, 13.4050),\n",
    "            'Munich': (48.1351, 11.5820),\n",
    "            'Frankfurt': (50.1109, 8.6821),\n",
    "            'Amsterdam': (52.3676, 4.9041),\n",
    "            'Brussels': (50.8503, 4.3517),\n",
    "            'Vienna': (48.2082, 16.3738),\n",
    "            'Zurich': (47.3769, 8.5417),\n",
    "            'Geneva': (46.2044, 6.1432),\n",
    "            'Stockholm': (59.3293, 18.0686),\n",
    "            'Oslo': (59.9139, 10.7522),\n",
    "            'Copenhagen': (55.6761, 12.5683),\n",
    "            'Helsinki': (60.1699, 24.9384),\n",
    "            'Warsaw': (52.2297, 21.0122),\n",
    "            'Prague': (50.0755, 14.4378),\n",
    "            'Budapest': (47.4979, 19.0402),\n",
    "            'Bucharest': (44.4268, 26.1025),\n",
    "            'Athens': (37.9838, 23.7275),\n",
    "            'Istanbul': (41.0082, 28.9784),\n",
    "            'Ankara': (39.9334, 32.8597),\n",
    "            \n",
    "            # OCEANIA\n",
    "            'Sydney': (33.8688, 151.2093),\n",
    "            'Melbourne': (37.8136, 144.9631),\n",
    "            'Brisbane': (27.4698, 153.0251),\n",
    "            'Perth': (31.9505, 115.8605),\n",
    "            'Adelaide': (34.9285, 138.6007),\n",
    "            'Canberra': (35.2809, 149.1300),\n",
    "            'Gold Coast': (28.0167, 153.4000),\n",
    "            'Newcastle': (32.9267, 151.7789),\n",
    "            'Auckland': (36.8485, 174.7633),\n",
    "            'Wellington': (41.2865, 174.7762),\n",
    "            'Christchurch': (43.5321, 172.6362),\n",
    "            \n",
    "            # AFRICA\n",
    "            'Lagos': (6.5244, 3.3792),\n",
    "            'Abuja': (9.0765, 7.3986),\n",
    "            'Kano': (12.0022, 8.5920),\n",
    "            'Ibadan': (7.3775, 3.9470),\n",
    "            'Cairo': (30.0444, 31.2357),\n",
    "            'Alexandria': (31.2001, 29.9187),\n",
    "            'Cape Town': (33.9249, 18.4241),\n",
    "            'Johannesburg': (26.2041, 28.0473),\n",
    "            'Durban': (29.8587, 31.0218),\n",
    "            'Nairobi': (1.2921, 36.8219),\n",
    "            'Addis Ababa': (9.1450, 38.7451),\n",
    "            'Casablanca': (33.5731, 7.5898),\n",
    "            'Tunis': (36.8065, 10.1815),\n",
    "            'Algiers': (36.7538, 3.0588),\n",
    "            \n",
    "            # SOUTH AMERICA\n",
    "            'S√£o Paulo': (23.5558, 46.6396),\n",
    "            'Rio de Janeiro': (22.9068, 43.1729),\n",
    "            'Bras√≠lia': (15.8267, 47.9218),\n",
    "            'Salvador': (12.9714, 38.5014),\n",
    "            'Buenos Aires': (34.6118, 58.3960),\n",
    "            'C√≥rdoba': (31.4201, 64.1888),\n",
    "            'Lima': (12.0464, 77.0428),\n",
    "            'Bogot√°': (4.7110, 74.0721),\n",
    "            'Medell√≠n': (6.2486, 75.5636),\n",
    "            'Caracas': (10.4806, 66.9036),\n",
    "            'Santiago': (33.4489, 70.6693),\n",
    "            'Quito': (0.1807, 78.4678),\n",
    "            'La Paz': (16.5000, 68.1193),\n",
    "            'Montevideo': (34.9011, 56.1645),\n",
    "            \n",
    "            # HIGH-RISK CYBERCRIME LOCATIONS\n",
    "            'Moscow': (55.7558, 37.6176),\n",
    "            'St. Petersburg': (59.9311, 30.3609),\n",
    "            'Novosibirsk': (55.0084, 82.9357),\n",
    "            'Yekaterinburg': (56.8431, 60.6454),\n",
    "            'Beijing': (39.9042, 116.4074),\n",
    "            'Shanghai': (31.2304, 121.4737),\n",
    "            'Shenzhen': (22.5431, 114.0579),\n",
    "            'Guangzhou': (23.1291, 113.2644),\n",
    "            'Hangzhou': (30.2741, 120.1551),\n",
    "            'Chengdu': (30.5728, 104.0668),\n",
    "            'Pyongyang': (39.0392, 125.7625),\n",
    "            'Hamhung': (39.9187, 127.5358),\n",
    "            'Chongjin': (41.7847, 129.7755),\n",
    "            'Minsk': (53.9006, 27.5590),\n",
    "            'Kiev': (50.4501, 30.5234),\n",
    "            'Kharkiv': (49.9935, 36.2304),\n",
    "            'Chisinau': (47.0105, 28.8638),\n",
    "            'Tirana': (41.3275, 19.8187),\n",
    "            'Skopje': (41.9973, 21.4280),\n",
    "            'Sarajevo': (43.8486, 18.3564),\n",
    "            \n",
    "            # ADDITIONAL ASIAN CITIES\n",
    "            'Mumbai': (19.0760, 72.8777),\n",
    "            'Delhi': (28.7041, 77.1025),\n",
    "            'Bangalore': (12.9716, 77.5946),\n",
    "            'Chennai': (13.0827, 80.2707),\n",
    "            'Hyderabad': (17.3850, 78.4867),\n",
    "            'Kolkata': (22.5726, 88.3639),\n",
    "            'Pune': (18.5204, 73.8567),\n",
    "            'Ahmedabad': (23.0225, 72.5714),\n",
    "            'Karachi': (24.8607, 67.0011),\n",
    "            'Lahore': (31.5204, 74.3587),\n",
    "            'Islamabad': (33.7294, 73.0931),\n",
    "            'Kabul': (34.5553, 69.2075),\n",
    "            'Tashkent': (41.2995, 69.2401),\n",
    "            'Almaty': (43.2220, 76.8512),\n",
    "            'Bishkek': (42.8746, 74.5698),\n",
    "            'Dushanbe': (38.5598, 68.7870),\n",
    "        }\n",
    "    \n",
    "    def _define_travel_zones(self):\n",
    "        \"\"\"\n",
    "        Define comprehensive risk zones for different types of travel destinations\n",
    "        Based on Filipino travel patterns, OFW destinations, and threat intelligence\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'ofw_hubs': {\n",
    "                'locations': [\n",
    "                    # Middle East - Major OFW employment destinations\n",
    "                    'Dubai', 'Abu Dhabi', 'Sharjah', 'Ajman', 'Al Ain',\n",
    "                    'Riyadh', 'Jeddah', 'Dammam', 'Mecca', 'Medina',\n",
    "                    'Doha', 'Kuwait City', 'Manama', 'Muscat'\n",
    "                ],\n",
    "                'base_risk': 0.2,  # Low risk for legitimate OFW destinations\n",
    "                'description': 'Major OFW employment hubs in Middle East'\n",
    "            },\n",
    "            'business_hubs': {\n",
    "                'locations': [\n",
    "                    # Asia Pacific business centers\n",
    "                    'Singapore', 'Hong Kong', 'Macau',\n",
    "                    'Tokyo', 'Osaka', 'Nagoya', 'Kyoto', 'Yokohama',\n",
    "                    'Seoul', 'Busan', 'Incheon',\n",
    "                    'Bangkok', 'Phuket', 'Kuala Lumpur', 'Penang',\n",
    "                    'Jakarta', 'Bali', 'Surabaya',\n",
    "                    # Global financial centers\n",
    "                    'London', 'Frankfurt', 'Zurich', 'Geneva',\n",
    "                    'New York', 'Chicago', 'Boston', 'San Francisco'\n",
    "                ],\n",
    "                'base_risk': 0.25,\n",
    "                'description': 'Regional and global business centers'\n",
    "            },\n",
    "            'diaspora_hubs': {\n",
    "                'locations': [\n",
    "                    # United States - Large Filipino communities\n",
    "                    'Los Angeles', 'San Francisco', 'San Diego', 'San Jose',\n",
    "                    'Las Vegas', 'Seattle', 'New York', 'Jersey City',\n",
    "                    'Chicago', 'Houston', 'Miami', 'Honolulu',\n",
    "                    # Canada - Filipino diaspora\n",
    "                    'Toronto', 'Vancouver', 'Montreal', 'Calgary', 'Edmonton',\n",
    "                    # Australia/New Zealand - Filipino communities\n",
    "                    'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide',\n",
    "                    'Auckland', 'Wellington'\n",
    "                ],\n",
    "                'base_risk': 0.3,\n",
    "                'description': 'Major Filipino diaspora communities'\n",
    "            },\n",
    "            'tourism_destinations': {\n",
    "                'locations': [\n",
    "                    # Europe - Common tourist destinations\n",
    "                    'Paris', 'Lyon', 'Rome', 'Milan', 'Venice', 'Naples',\n",
    "                    'Madrid', 'Barcelona', 'Berlin', 'Munich',\n",
    "                    'Amsterdam', 'Brussels', 'Vienna',\n",
    "                    'Stockholm', 'Oslo', 'Copenhagen', 'Helsinki',\n",
    "                    'Prague', 'Budapest', 'Warsaw', 'Athens',\n",
    "                    'Istanbul', 'Dublin', 'Edinburgh',\n",
    "                    # Other popular destinations\n",
    "                    'Ho Chi Minh City', 'Hanoi', 'Da Nang',\n",
    "                    'Phnom Penh', 'Vientiane', 'Yangon',\n",
    "                    'Mumbai', 'Delhi', 'Bangalore'\n",
    "                ],\n",
    "                'base_risk': 0.35,\n",
    "                'description': 'Popular tourist and cultural destinations'\n",
    "            },\n",
    "            'developing_markets': {\n",
    "                'locations': [\n",
    "                    # South/Southeast Asia\n",
    "                    'Chennai', 'Hyderabad', 'Kolkata', 'Pune', 'Ahmedabad',\n",
    "                    'Karachi', 'Lahore', 'Islamabad', 'Dhaka',\n",
    "                    'Colombo', 'Kathmandu', 'Kabul',\n",
    "                    # Central Asia\n",
    "                    'Tashkent', 'Almaty', 'Bishkek', 'Dushanbe',\n",
    "                    # Africa\n",
    "                    'Cairo', 'Alexandria', 'Cape Town', 'Johannesburg',\n",
    "                    'Nairobi', 'Addis Ababa', 'Casablanca',\n",
    "                    # South America\n",
    "                    'S√£o Paulo', 'Rio de Janeiro', 'Buenos Aires', 'Lima'\n",
    "                ],\n",
    "                'base_risk': 0.45,\n",
    "                'description': 'Developing markets with moderate risk'\n",
    "            },\n",
    "            'high_risk_regions': {\n",
    "                'locations': [\n",
    "                    # Africa - Higher risk areas\n",
    "                    'Lagos', 'Abuja', 'Kano', 'Ibadan',\n",
    "                    'Tunis', 'Algiers',\n",
    "                    # South America - Crime hotspots\n",
    "                    'Bogot√°', 'Medell√≠n', 'Caracas', 'La Paz',\n",
    "                    # Conflict zones\n",
    "                    'Baghdad', 'Beirut', 'Amman'\n",
    "                ],\n",
    "                'base_risk': 0.65,\n",
    "                'description': 'Higher risk regions with security concerns'\n",
    "            },\n",
    "            'cybercrime_hubs': {\n",
    "                'locations': [\n",
    "                    # Russia - Major cybercrime source\n",
    "                    'Moscow', 'St. Petersburg', 'Novosibirsk', 'Yekaterinburg',\n",
    "                    # China - State-sponsored threats\n",
    "                    'Beijing', 'Shanghai', 'Shenzhen', 'Guangzhou', \n",
    "                    'Hangzhou', 'Chengdu',\n",
    "                    # North Korea - State actors\n",
    "                    'Pyongyang', 'Hamhung', 'Chongjin',\n",
    "                    # Iran - Cyber warfare\n",
    "                    'Tehran', 'Isfahan', 'Mashhad',\n",
    "                    # Eastern Europe - Cybercrime centers\n",
    "                    'Bucharest', 'Minsk', 'Kiev', 'Kharkiv',\n",
    "                    'Chisinau', 'Tirana', 'Skopje', 'Sarajevo'\n",
    "                ],\n",
    "                'base_risk': 0.8,\n",
    "                'description': 'Known cybercrime and state-sponsored threat locations'\n",
    "            },\n",
    "            'philippines_domestic': {\n",
    "                'locations': [\n",
    "                    # Metro Manila\n",
    "                    'Manila', 'Quezon City', 'Makati', 'Taguig', 'Pasig',\n",
    "                    'Mandaluyong', 'Marikina', 'Pasay', 'Para√±aque',\n",
    "                    'Las Pi√±as', 'Muntinlupa', 'Caloocan', 'Valenzuela',\n",
    "                    'Malabon', 'Navotas',\n",
    "                    # Luzon\n",
    "                    'Baguio', 'Angeles', 'San Fernando', 'Dagupan',\n",
    "                    'Cabanatuan', 'Olongapo', 'Batangas', 'Lipa',\n",
    "                    'Lucena', 'Naga', 'Legazpi', 'Vigan', 'Tuguegarao', 'Laoag',\n",
    "                    # Visayas\n",
    "                    'Cebu City', 'Mandaue', 'Lapu-Lapu', 'Iloilo City',\n",
    "                    'Bacolod', 'Dumaguete', 'Tacloban', 'Ormoc',\n",
    "                    'Tagbilaran', 'Roxas', 'Kalibo',\n",
    "                    # Mindanao\n",
    "                    'Davao City', 'Cagayan de Oro', 'Zamboanga', 'Butuan',\n",
    "                    'Iligan', 'Cotabato', 'General Santos', 'Koronadal',\n",
    "                    'Kidapawan', 'Dipolog', 'Pagadian', 'Marawi'\n",
    "                ],\n",
    "                'base_risk': 0.05,  # Very low risk for domestic access\n",
    "                'description': 'Philippine domestic locations'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_user_baseline(self, user_login_history):\n",
    "        \"\"\"\n",
    "        Analyze user's normal behavior patterns from login history\n",
    "        \n",
    "        Args:\n",
    "            user_login_history: List of dictionaries with login data\n",
    "        \"\"\"\n",
    "        user_id = user_login_history[0]['user_id']\n",
    "        \n",
    "        # Extract behavioral patterns\n",
    "        locations = [login['location'] for login in user_login_history]\n",
    "        times = [datetime.strptime(login['timestamp'], '%Y-%m-%d %H:%M:%S') for login in user_login_history]\n",
    "        devices = [login['device_type'] for login in user_login_history]\n",
    "        countries = [login['country'] for login in user_login_history]\n",
    "        \n",
    "        # Calculate baseline patterns\n",
    "        baseline = {\n",
    "            'user_id': user_id,\n",
    "            'home_locations': list(set(loc for loc, country in zip(locations, countries) if country == 'PH')),\n",
    "            'common_devices': list(set(devices)),\n",
    "            'typical_hours': [t.hour for t in times],\n",
    "            'login_frequency': len(user_login_history),\n",
    "            'countries_visited': list(set(countries)),\n",
    "            'last_known_location': locations[-1],\n",
    "            'last_login_time': times[-1],\n",
    "            'travel_history': self._extract_travel_history(user_login_history)\n",
    "        }\n",
    "        \n",
    "        # Store user profile\n",
    "        self.user_profiles[user_id] = baseline\n",
    "        \n",
    "        print(f\"‚úÖ Baseline established for User {user_id}\")\n",
    "        print(f\"   Home locations: {baseline['home_locations']}\")\n",
    "        print(f\"   Countries visited: {baseline['countries_visited']}\")\n",
    "        print(f\"   Common devices: {baseline['common_devices']}\")\n",
    "        \n",
    "        return baseline\n",
    "    \n",
    "    def _extract_travel_history(self, login_history):\n",
    "        \"\"\"\n",
    "        Extract travel patterns from login history\n",
    "        \"\"\"\n",
    "        travels = []\n",
    "        \n",
    "        for i in range(1, len(login_history)):\n",
    "            prev_login = login_history[i-1]\n",
    "            curr_login = login_history[i]\n",
    "            \n",
    "            if prev_login['country'] != curr_login['country']:\n",
    "                travel = {\n",
    "                    'from_location': prev_login['location'],\n",
    "                    'to_location': curr_login['location'],\n",
    "                    'from_country': prev_login['country'],\n",
    "                    'to_country': curr_login['country'],\n",
    "                    'time_gap': (datetime.strptime(curr_login['timestamp'], '%Y-%m-%d %H:%M:%S') - \n",
    "                               datetime.strptime(prev_login['timestamp'], '%Y-%m-%d %H:%M:%S')).total_seconds() / 3600,\n",
    "                    'distance_km': self._calculate_distance(prev_login['location'], curr_login['location'])\n",
    "                }\n",
    "                travels.append(travel)\n",
    "        \n",
    "        return travels\n",
    "    \n",
    "    def _calculate_distance(self, location1, location2):\n",
    "        \"\"\"\n",
    "        Calculate distance between two locations\n",
    "        \"\"\"\n",
    "        if location1 in self.location_coordinates and location2 in self.location_coordinates:\n",
    "            coord1 = self.location_coordinates[location1]\n",
    "            coord2 = self.location_coordinates[location2]\n",
    "            return geodesic(coord1, coord2).kilometers\n",
    "        return 0\n",
    "    \n",
    "    def _get_location_zone(self, location):\n",
    "        \"\"\"\n",
    "        Determine which risk zone a location belongs to\n",
    "        \"\"\"\n",
    "        for zone_name, zone_data in self.travel_risk_zones.items():\n",
    "            if location in zone_data['locations']:\n",
    "                return zone_name, zone_data['base_risk']\n",
    "        return 'unknown', 0.5  # Default for unknown locations\n",
    "    \n",
    "    def analyze_travel_plausibility(self, user_id, new_login):\n",
    "        \"\"\"\n",
    "        Analyze if travel to new location is physically plausible\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return {\n",
    "                'plausible': False,\n",
    "                'reason': 'No user baseline established',\n",
    "                'risk_modifier': 0.5\n",
    "            }\n",
    "        \n",
    "        profile = self.user_profiles[user_id]\n",
    "        last_location = profile['last_known_location']\n",
    "        last_time = profile['last_login_time']\n",
    "        \n",
    "        new_location = new_login['location']\n",
    "        new_time = datetime.strptime(new_login['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Calculate travel requirements\n",
    "        distance_km = self._calculate_distance(last_location, new_location)\n",
    "        time_gap_hours = (new_time - last_time).total_seconds() / 3600\n",
    "        \n",
    "        if distance_km == 0:  # Same location or unknown coordinates\n",
    "            return {\n",
    "                'plausible': True,\n",
    "                'reason': 'Same location or local area',\n",
    "                'risk_modifier': 0.0,\n",
    "                'distance_km': distance_km,\n",
    "                'time_gap_hours': time_gap_hours\n",
    "            }\n",
    "        \n",
    "        # Calculate minimum travel time (assuming commercial flight)\n",
    "        min_travel_time_hours = distance_km / self.max_travel_speed_kmh\n",
    "        buffer_time_hours = 4  # Airport procedures, layovers, etc.\n",
    "        required_time_hours = min_travel_time_hours + buffer_time_hours\n",
    "        \n",
    "        # Check plausibility\n",
    "        if time_gap_hours >= required_time_hours:\n",
    "            return {\n",
    "                'plausible': True,\n",
    "                'reason': f'Sufficient time for travel ({time_gap_hours:.1f}h vs {required_time_hours:.1f}h required)',\n",
    "                'risk_modifier': 0.0,\n",
    "                'distance_km': distance_km,\n",
    "                'time_gap_hours': time_gap_hours,\n",
    "                'required_time_hours': required_time_hours\n",
    "            }\n",
    "        else:\n",
    "            # Impossible travel\n",
    "            return {\n",
    "                'plausible': False,\n",
    "                'reason': f'Impossible travel: {distance_km:.0f}km in {time_gap_hours:.1f}h (need {required_time_hours:.1f}h)',\n",
    "                'risk_modifier': 0.8,  # Very high risk for impossible travel\n",
    "                'distance_km': distance_km,\n",
    "                'time_gap_hours': time_gap_hours,\n",
    "                'required_time_hours': required_time_hours\n",
    "            }\n",
    "    \n",
    "    def analyze_behavioral_consistency(self, user_id, new_login):\n",
    "        \"\"\"\n",
    "        Check if user behavior remains consistent despite location change\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return {'consistency_score': 0.5, 'factors': ['No baseline']}\n",
    "        \n",
    "        profile = self.user_profiles[user_id]\n",
    "        consistency_factors = []\n",
    "        consistency_score = 1.0  # Start with perfect consistency\n",
    "        \n",
    "        # Device consistency\n",
    "        if new_login['device_type'] in profile['common_devices']:\n",
    "            consistency_factors.append('Known device type')\n",
    "        else:\n",
    "            consistency_score -= 0.3\n",
    "            consistency_factors.append('New device type')\n",
    "        \n",
    "        # Time pattern consistency (adjusted for timezone)\n",
    "        new_hour = datetime.strptime(new_login['timestamp'], '%Y-%m-%d %H:%M:%S').hour\n",
    "        if new_hour in profile['typical_hours'] or abs(new_hour - np.mean(profile['typical_hours'])) <= 3:\n",
    "            consistency_factors.append('Consistent login time')\n",
    "        else:\n",
    "            consistency_score -= 0.2\n",
    "            consistency_factors.append('Unusual login time')\n",
    "        \n",
    "        # Previous travel history\n",
    "        if new_login['country'] in profile['countries_visited']:\n",
    "            consistency_factors.append('Previously visited country')\n",
    "            consistency_score += 0.1  # Bonus for familiar destinations\n",
    "        else:\n",
    "            consistency_factors.append('First visit to country')\n",
    "        \n",
    "        # Browser/session consistency (if available)\n",
    "        # This would check if session tokens, browser fingerprints match\n",
    "        # For demo, we'll simulate this\n",
    "        if np.random.random() > 0.3:  # 70% chance of consistent browser\n",
    "            consistency_factors.append('Consistent browser fingerprint')\n",
    "        else:\n",
    "            consistency_score -= 0.2\n",
    "            consistency_factors.append('Different browser fingerprint')\n",
    "        \n",
    "        return {\n",
    "            'consistency_score': max(0, min(1, consistency_score)),\n",
    "            'factors': consistency_factors\n",
    "        }\n",
    "    \n",
    "    def calculate_travel_aware_risk(self, user_id, new_login):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive travel-aware risk score\n",
    "        \"\"\"\n",
    "        # Base risk components\n",
    "        risk_components = {}\n",
    "        \n",
    "        # 1. Location zone risk\n",
    "        zone, base_location_risk = self._get_location_zone(new_login['location'])\n",
    "        risk_components['location_zone'] = base_location_risk\n",
    "        \n",
    "        # 2. Travel plausibility\n",
    "        travel_analysis = self.analyze_travel_plausibility(user_id, new_login)\n",
    "        risk_components['travel_plausibility'] = travel_analysis['risk_modifier']\n",
    "        \n",
    "        # 3. Behavioral consistency\n",
    "        behavior_analysis = self.analyze_behavioral_consistency(user_id, new_login)\n",
    "        behavior_risk = 1 - behavior_analysis['consistency_score']\n",
    "        risk_components['behavioral_inconsistency'] = behavior_risk\n",
    "        \n",
    "        # 4. Technical indicators\n",
    "        technical_risk = 0.0\n",
    "        technical_factors = []\n",
    "        \n",
    "        if new_login.get('is_attack_ip', False):\n",
    "            technical_risk += 0.4\n",
    "            technical_factors.append('Known attack IP')\n",
    "        \n",
    "        if new_login.get('high_latency', False):\n",
    "            technical_risk += 0.2\n",
    "            technical_factors.append('High network latency')\n",
    "        \n",
    "        if not new_login.get('login_successful', True):\n",
    "            technical_risk += 0.3\n",
    "            technical_factors.append('Failed login attempt')\n",
    "        \n",
    "        risk_components['technical_indicators'] = technical_risk\n",
    "        \n",
    "        # 5. Geographic distance factor\n",
    "        if user_id in self.user_profiles:\n",
    "            distance_km = self._calculate_distance(\n",
    "                self.user_profiles[user_id]['last_known_location'],\n",
    "                new_login['location']\n",
    "            )\n",
    "            # Risk increases with distance, but caps at 0.3\n",
    "            distance_risk = min(0.3, distance_km / 10000)  # 10,000km = max distance risk\n",
    "            risk_components['distance'] = distance_risk\n",
    "        else:\n",
    "            risk_components['distance'] = 0.2\n",
    "        \n",
    "        # Calculate weighted final risk score\n",
    "        weights = {\n",
    "            'location_zone': 0.3,\n",
    "            'travel_plausibility': 0.25,\n",
    "            'behavioral_inconsistency': 0.2,\n",
    "            'technical_indicators': 0.15,\n",
    "            'distance': 0.1\n",
    "        }\n",
    "        \n",
    "        final_risk = sum(risk_components[component] * weights[component] \n",
    "                        for component in risk_components)\n",
    "        \n",
    "        # Cap at 1.0\n",
    "        final_risk = min(1.0, final_risk)\n",
    "        \n",
    "        return {\n",
    "            'final_risk_score': final_risk,\n",
    "            'risk_components': risk_components,\n",
    "            'travel_analysis': travel_analysis,\n",
    "            'behavior_analysis': behavior_analysis,\n",
    "            'location_zone': zone,\n",
    "            'technical_factors': technical_factors\n",
    "        }\n",
    "    \n",
    "    def generate_travel_aware_explanation(self, risk_analysis, language='english'):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation for the risk decision\n",
    "        \"\"\"\n",
    "        risk_score = risk_analysis['final_risk_score']\n",
    "        travel_info = risk_analysis['travel_analysis']\n",
    "        behavior_info = risk_analysis['behavior_analysis']\n",
    "        \n",
    "        # Determine risk level\n",
    "        if risk_score < 0.3:\n",
    "            risk_level = \"LOW\"\n",
    "            action = \"ALLOW\"\n",
    "            message_key = \"low_risk\"\n",
    "        elif risk_score < 0.6:\n",
    "            risk_level = \"MEDIUM\"\n",
    "            action = \"ALLOW_WITH_OTP\"\n",
    "            message_key = \"medium_risk\"\n",
    "        else:\n",
    "            risk_level = \"HIGH\"\n",
    "            action = \"BLOCK\" if not travel_info['plausible'] else \"STRICT_VERIFICATION\"\n",
    "            message_key = \"high_risk\"\n",
    "        \n",
    "        # Build explanation factors\n",
    "        explanation_factors = []\n",
    "        \n",
    "        # Travel plausibility\n",
    "        if travel_info['plausible']:\n",
    "            explanation_factors.append(f\"‚úÖ Travel is plausible ({travel_info['reason']})\")\n",
    "        else:\n",
    "            explanation_factors.append(f\"‚ùå {travel_info['reason']}\")\n",
    "        \n",
    "        # Behavioral consistency\n",
    "        consistency_pct = behavior_info['consistency_score'] * 100\n",
    "        explanation_factors.append(f\"üîç Behavior consistency: {consistency_pct:.0f}%\")\n",
    "        \n",
    "        # Location zone\n",
    "        zone = risk_analysis['location_zone']\n",
    "        zone_info = self.travel_risk_zones.get(zone, {})\n",
    "        explanation_factors.append(f\"üìç Location: {zone_info.get('description', 'Unknown zone')}\")\n",
    "        \n",
    "        # Technical factors\n",
    "        if risk_analysis['technical_factors']:\n",
    "            explanation_factors.extend([f\"‚ö†Ô∏è {factor}\" for factor in risk_analysis['technical_factors']])\n",
    "        \n",
    "        return {\n",
    "            'risk_score': risk_score,\n",
    "            'risk_level': risk_level,\n",
    "            'action': action,\n",
    "            'explanation_factors': explanation_factors,\n",
    "            'travel_plausible': travel_info['plausible'],\n",
    "            'behavior_consistent': behavior_info['consistency_score'] > 0.7,\n",
    "            'recommendation': self._get_recommendation(risk_score, travel_info, behavior_info)\n",
    "        }\n",
    "    \n",
    "    def _get_recommendation(self, risk_score, travel_info, behavior_info):\n",
    "        \"\"\"\n",
    "        Generate specific recommendations based on analysis\n",
    "        \"\"\"\n",
    "        if not travel_info['plausible']:\n",
    "            return \"BLOCK: Impossible travel detected. Manual review required.\"\n",
    "        \n",
    "        if risk_score < 0.3 and behavior_info['consistency_score'] > 0.8:\n",
    "            return \"ALLOW: Legitimate travel with consistent behavior.\"\n",
    "        \n",
    "        if risk_score < 0.6 and travel_info['plausible']:\n",
    "            return \"ALLOW with SMS OTP: Possible legitimate travel, verify with additional authentication.\"\n",
    "        \n",
    "        return \"STRICT VERIFICATION: High-risk login requiring manual review and multiple authentication factors.\"\n",
    "\n",
    "def demo_travel_scenarios():\n",
    "    \"\"\"\n",
    "    Demonstrate travel-aware RBA with realistic scenarios\n",
    "    \"\"\"\n",
    "    print(\"üõ´ BANTAI TRAVEL-AWARE RBA DEMO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize the travel-aware system\n",
    "    bantai_travel = BantAI_TravelAware()\n",
    "    \n",
    "    # Sample user: Juan, an OFW from Manila\n",
    "    user_id = \"juan_dela_cruz_123\"\n",
    "    \n",
    "    # Juan's login history (establishing baseline)\n",
    "    juan_history = [\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-01 09:00:00',\n",
    "            'location': 'Manila',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-03 14:30:00',\n",
    "            'location': 'Makati',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-05 11:15:00',\n",
    "            'location': 'Quezon City',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'desktop',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-08 16:45:00',\n",
    "            'location': 'Manila',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-10 08:20:00',\n",
    "            'location': 'Manila',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Establish baseline\n",
    "    print(\"\\nüìä ESTABLISHING USER BASELINE...\")\n",
    "    baseline = bantai_travel.analyze_user_baseline(juan_history)\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            'name': '‚úàÔ∏è Legitimate OFW Travel to Dubai',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-01-20 19:30:00',  # 10 days later, enough time to travel\n",
    "                'location': 'Dubai',\n",
    "                'country': 'AE',\n",
    "                'device_type': 'mobile',  # Same device\n",
    "                'login_successful': True,\n",
    "                'is_attack_ip': False,\n",
    "                'high_latency': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'üö® Impossible Travel Attack',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-01-20 20:30:00',  # 1 hour after Dubai login\n",
    "                'location': 'Moscow',\n",
    "                'country': 'RU',\n",
    "                'device_type': 'desktop',  # Different device\n",
    "                'login_successful': False,\n",
    "                'is_attack_ip': True,\n",
    "                'high_latency': True\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'üõÇ Business Trip to Singapore',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-01-25 10:00:00',  # 5 days later from Dubai\n",
    "                'location': 'Singapore',\n",
    "                'country': 'SG',\n",
    "                'device_type': 'mobile',  # Same device\n",
    "                'login_successful': True,\n",
    "                'is_attack_ip': False,\n",
    "                'high_latency': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'üë®‚Äçüíº Return to Philippines',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-02-01 15:00:00',  # 1 week later\n",
    "                'location': 'Manila',\n",
    "                'country': 'PH',\n",
    "                'device_type': 'mobile',\n",
    "                'login_successful': True,\n",
    "                'is_attack_ip': False,\n",
    "                'high_latency': False\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Analyze each scenario\n",
    "    print(f\"\\nüîç ANALYZING TRAVEL SCENARIOS...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\n{scenario['name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Calculate risk\n",
    "        risk_analysis = bantai_travel.calculate_travel_aware_risk(\n",
    "            user_id, scenario['login']\n",
    "        )\n",
    "        \n",
    "        # Generate explanation\n",
    "        explanation = bantai_travel.generate_travel_aware_explanation(risk_analysis)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üìç Location: {scenario['login']['location']}, {scenario['login']['country']}\")\n",
    "        print(f\"üéØ Risk Score: {explanation['risk_score']:.2f} ({explanation['risk_level']})\")\n",
    "        print(f\"‚ö° Action: {explanation['action']}\")\n",
    "        print(f\"‚úÖ Travel Plausible: {explanation['travel_plausible']}\")\n",
    "        print(f\"üîÑ Behavior Consistent: {explanation['behavior_consistent']}\")\n",
    "        print(f\"üí° Recommendation: {explanation['recommendation']}\")\n",
    "        \n",
    "        print(\"\\nüîç Detailed Analysis:\")\n",
    "        for factor in explanation['explanation_factors']:\n",
    "            print(f\"   {factor}\")\n",
    "    \n",
    "    return bantai_travel\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the travel-aware demo\n",
    "    travel_system = demo_travel_scenarios()\n",
    "    \n",
    "    print(\"\\nüí° Usage for Custom Scenarios:\")\n",
    "    print(\"# 1. Establish user baseline:\")\n",
    "    print(\"baseline = travel_system.analyze_user_baseline(user_login_history)\")\n",
    "    print(\"\\n# 2. Analyze new login:\")\n",
    "    print(\"risk_analysis = travel_system.calculate_travel_aware_risk(user_id, new_login)\")\n",
    "    print(\"explanation = travel_system.generate_travel_aware_explanation(risk_analysis)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BantAI Travel-Aware Risk-Based Authentication System\n",
    "Separate implementation for intelligent travel vs threat detection\n",
    "\n",
    "Distinguishes between:\n",
    "- Legitimate Filipino travelers (OFWs, tourists, business)\n",
    "- Account compromise/cyber attacks\n",
    "\n",
    "Features:\n",
    "- Travel plausibility analysis\n",
    "- Behavioral consistency scoring\n",
    "- Impossible travel detection\n",
    "- OFW-friendly risk assessment\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from geopy.distance import geodesic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BantAI_TravelAware:\n",
    "    \"\"\"\n",
    "    Travel-Aware Risk-Based Authentication for Filipino Banking\n",
    "    \n",
    "    Smart enough to distinguish between:\n",
    "    - Juan traveling to Dubai for work (legitimate)\n",
    "    - Hacker accessing from Moscow (threat)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.user_profiles = {}  # Store user behavioral baselines\n",
    "        self.location_coordinates = self._load_location_data()\n",
    "        self.travel_risk_zones = self._define_travel_zones()\n",
    "        self.max_travel_speed_kmh = 900  # Commercial aircraft speed\n",
    "        \n",
    "    def _load_location_data(self):\n",
    "        \"\"\"\n",
    "        Comprehensive geographic coordinates for travel distance calculations\n",
    "        Covering major Philippine cities and global destinations\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # PHILIPPINES - Major Cities and Regions\n",
    "            'Manila': (14.5995, 120.9842),\n",
    "            'Quezon City': (14.6760, 121.0437),\n",
    "            'Makati': (14.5547, 121.0244),\n",
    "            'Taguig': (14.5176, 121.0509),\n",
    "            'Pasig': (14.5764, 121.0851),\n",
    "            'Mandaluyong': (14.5794, 121.0359),\n",
    "            'Marikina': (14.6507, 121.1029),\n",
    "            'Pasay': (14.5378, 120.9896),\n",
    "            'Para√±aque': (14.4793, 121.0198),\n",
    "            'Las Pi√±as': (14.4304, 120.9820),\n",
    "            'Muntinlupa': (14.4037, 121.0270),\n",
    "            'Caloocan': (14.6488, 120.9658),\n",
    "            'Valenzuela': (14.6958, 120.9830),\n",
    "            'Malabon': (14.6570, 120.9658),\n",
    "            'Navotas': (14.6691, 120.9618),\n",
    "            \n",
    "            # Luzon Cities\n",
    "            'Baguio': (16.4023, 120.5960),\n",
    "            'Angeles': (15.1450, 120.5864),\n",
    "            'San Fernando': (15.0332, 120.6833),\n",
    "            'Dagupan': (16.0433, 120.3433),\n",
    "            'Cabanatuan': (15.4891, 120.9627),\n",
    "            'Olongapo': (14.8294, 120.2824),\n",
    "            'Batangas': (13.7565, 121.0583),\n",
    "            'Lipa': (13.9411, 121.1634),\n",
    "            'Lucena': (13.9373, 121.617),\n",
    "            'Naga': (13.6218, 123.1948),\n",
    "            'Legazpi': (13.1391, 123.7437),\n",
    "            'Iloilo City': (10.7202, 122.5621),\n",
    "            'Vigan': (17.5748, 120.3871),\n",
    "            'Tuguegarao': (17.6132, 121.7270),\n",
    "            'Laoag': (18.1967, 120.5929),\n",
    "            \n",
    "            # Visayas Cities\n",
    "            'Cebu City': (10.3157, 123.8854),\n",
    "            'Mandaue': (10.3237, 123.9227),\n",
    "            'Lapu-Lapu': (10.3103, 123.9494),\n",
    "            'Bacolod': (10.6740, 122.9540),\n",
    "            'Dumaguete': (9.3067, 123.3065),\n",
    "            'Tacloban': (11.2447, 125.0048),\n",
    "            'Ormoc': (11.0059, 124.6074),\n",
    "            'Tagbilaran': (9.6496, 123.8543),\n",
    "            'Roxas': (11.5854, 122.7511),\n",
    "            'Kalibo': (11.7040, 122.3690),\n",
    "            \n",
    "            # Mindanao Cities\n",
    "            'Davao City': (7.1907, 125.4553),\n",
    "            'Cagayan de Oro': (8.4542, 124.6319),\n",
    "            'Zamboanga': (6.9214, 122.0790),\n",
    "            'Butuan': (8.9470, 125.5406),\n",
    "            'Iligan': (8.2280, 124.2452),\n",
    "            'Cotabato': (7.2231, 124.2467),\n",
    "            'General Santos': (6.1164, 125.1716),\n",
    "            'Koronadal': (6.5000, 124.8500),\n",
    "            'Kidapawan': (7.0103, 125.0890),\n",
    "            'Dipolog': (8.5958, 123.3417),\n",
    "            'Pagadian': (7.8272, 123.4433),\n",
    "            'Marawi': (8.0021, 124.2979),\n",
    "            \n",
    "            # MIDDLE EAST - Major OFW Destinations\n",
    "            'Dubai': (25.2048, 55.2708),\n",
    "            'Abu Dhabi': (24.4539, 54.3773),\n",
    "            'Sharjah': (25.3573, 55.4033),\n",
    "            'Ajman': (25.4052, 55.5136),\n",
    "            'Al Ain': (24.2075, 55.7647),\n",
    "            'Riyadh': (24.7136, 46.6753),\n",
    "            'Jeddah': (21.4858, 39.1925),\n",
    "            'Dammam': (26.4207, 50.0888),\n",
    "            'Mecca': (21.3891, 39.8579),\n",
    "            'Medina': (24.5247, 39.5692),\n",
    "            'Doha': (25.2854, 51.5310),\n",
    "            'Kuwait City': (29.3117, 47.4818),\n",
    "            'Manama': (26.2285, 50.5860),\n",
    "            'Muscat': (23.5880, 58.3829),\n",
    "            'Amman': (31.9539, 35.9106),\n",
    "            'Beirut': (33.8938, 35.5018),\n",
    "            'Baghdad': (33.3152, 44.3661),\n",
    "            'Tehran': (35.6892, 51.3890),\n",
    "            'Isfahan': (32.6546, 51.6680),\n",
    "            'Mashhad': (36.2605, 59.6168),\n",
    "            \n",
    "            # ASIA PACIFIC - Business and Tourism Hubs\n",
    "            'Singapore': (1.3521, 103.8198),\n",
    "            'Hong Kong': (22.3193, 114.1694),\n",
    "            'Macau': (22.1987, 113.5439),\n",
    "            'Tokyo': (35.6762, 139.6503),\n",
    "            'Osaka': (34.6937, 135.5023),\n",
    "            'Nagoya': (35.1815, 136.9066),\n",
    "            'Kyoto': (35.0116, 135.7681),\n",
    "            'Yokohama': (35.4437, 139.6380),\n",
    "            'Seoul': (37.5665, 126.9780),\n",
    "            'Busan': (35.1796, 129.0756),\n",
    "            'Incheon': (37.4563, 126.7052),\n",
    "            'Bangkok': (13.7563, 100.5018),\n",
    "            'Phuket': (7.8804, 98.3923),\n",
    "            'Pattaya': (12.9236, 100.8825),\n",
    "            'Kuala Lumpur': (3.1390, 101.6869),\n",
    "            'Johor Bahru': (1.4927, 103.7414),\n",
    "            'Penang': (5.4164, 100.3327),\n",
    "            'Jakarta': (6.2088, 106.8456),\n",
    "            'Bali': (8.3405, 115.0920),\n",
    "            'Surabaya': (7.2575, 112.7521),\n",
    "            'Ho Chi Minh City': (10.8231, 106.6297),\n",
    "            'Hanoi': (21.0285, 105.8542),\n",
    "            'Da Nang': (16.0544, 108.2022),\n",
    "            'Phnom Penh': (11.5564, 104.9282),\n",
    "            'Vientiane': (17.9757, 102.6331),\n",
    "            'Yangon': (16.8661, 96.1951),\n",
    "            'Colombo': (6.9271, 79.8612),\n",
    "            'Dhaka': (23.8103, 90.4125),\n",
    "            'Kathmandu': (27.7172, 85.3240),\n",
    "            \n",
    "            # NORTH AMERICA - Filipino Diaspora Communities\n",
    "            'Los Angeles': (34.0522, -118.2437),\n",
    "            'San Francisco': (37.7749, -122.4194),\n",
    "            'San Diego': (32.7157, -117.1611),\n",
    "            'Las Vegas': (36.1699, -115.1398),\n",
    "            'Phoenix': (33.4484, -112.0740),\n",
    "            'Seattle': (47.6062, -122.3321),\n",
    "            'Portland': (45.5152, -122.6784),\n",
    "            'Sacramento': (38.5816, -121.4944),\n",
    "            'Fresno': (36.7378, -119.7871),\n",
    "            'San Jose': (37.3382, -121.8863),\n",
    "            'New York': (40.7128, -74.0060),\n",
    "            'Jersey City': (40.7178, -74.0431),\n",
    "            'Philadelphia': (39.9526, -75.1652),\n",
    "            'Washington DC': (38.9072, -77.0369),\n",
    "            'Boston': (42.3601, -71.0589),\n",
    "            'Chicago': (41.8781, -87.6298),\n",
    "            'Detroit': (42.3314, -83.0458),\n",
    "            'Miami': (25.7617, -80.1918),\n",
    "            'Orlando': (28.5383, -81.3792),\n",
    "            'Tampa': (27.9506, -82.4572),\n",
    "            'Houston': (29.7604, -95.3698),\n",
    "            'Dallas': (32.7767, -96.7970),\n",
    "            'Austin': (30.2672, -97.7431),\n",
    "            'San Antonio': (29.4241, -98.4936),\n",
    "            'Denver': (39.7392, -104.9903),\n",
    "            'Atlanta': (33.7490, -84.3880),\n",
    "            'Honolulu': (21.3099, -157.8581),\n",
    "            'Anchorage': (61.2181, -149.9003),\n",
    "            \n",
    "            # CANADA\n",
    "            'Toronto': (43.6532, -79.3832),\n",
    "            'Vancouver': (49.2827, -123.1207),\n",
    "            'Montreal': (45.5017, -73.5673),\n",
    "            'Calgary': (51.0447, -114.0719),\n",
    "            'Edmonton': (53.5461, -113.4938),\n",
    "            'Ottawa': (45.4215, -75.6972),\n",
    "            'Winnipeg': (49.8951, -97.1384),\n",
    "            'Quebec City': (46.8139, -71.2080),\n",
    "            'Hamilton': (43.2557, -79.8711),\n",
    "            \n",
    "            # EUROPE - Tourism and Business\n",
    "            'London': (51.5074, -0.1278),\n",
    "            'Manchester': (53.4808, -2.2426),\n",
    "            'Birmingham': (52.4862, -1.8904),\n",
    "            'Edinburgh': (55.9533, -3.1883),\n",
    "            'Glasgow': (55.8642, -4.2518),\n",
    "            'Dublin': (53.3498, -6.2603),\n",
    "            'Paris': (48.8566, 2.3522),\n",
    "            'Lyon': (45.7640, 4.8357),\n",
    "            'Marseille': (43.2965, 5.3698),\n",
    "            'Rome': (41.9028, 12.4964),\n",
    "            'Milan': (45.4642, 9.1900),\n",
    "            'Naples': (40.8518, 14.2681),\n",
    "            'Venice': (45.4408, 12.3155),\n",
    "            'Madrid': (40.4168, -3.7038),\n",
    "            'Barcelona': (41.3851, 2.1734),\n",
    "            'Berlin': (52.5200, 13.4050),\n",
    "            'Munich': (48.1351, 11.5820),\n",
    "            'Frankfurt': (50.1109, 8.6821),\n",
    "            'Amsterdam': (52.3676, 4.9041),\n",
    "            'Brussels': (50.8503, 4.3517),\n",
    "            'Vienna': (48.2082, 16.3738),\n",
    "            'Zurich': (47.3769, 8.5417),\n",
    "            'Geneva': (46.2044, 6.1432),\n",
    "            'Stockholm': (59.3293, 18.0686),\n",
    "            'Oslo': (59.9139, 10.7522),\n",
    "            'Copenhagen': (55.6761, 12.5683),\n",
    "            'Helsinki': (60.1699, 24.9384),\n",
    "            'Warsaw': (52.2297, 21.0122),\n",
    "            'Prague': (50.0755, 14.4378),\n",
    "            'Budapest': (47.4979, 19.0402),\n",
    "            'Bucharest': (44.4268, 26.1025),\n",
    "            'Athens': (37.9838, 23.7275),\n",
    "            'Istanbul': (41.0082, 28.9784),\n",
    "            'Ankara': (39.9334, 32.8597),\n",
    "            \n",
    "            # OCEANIA\n",
    "            'Sydney': (33.8688, 151.2093),\n",
    "            'Melbourne': (37.8136, 144.9631),\n",
    "            'Brisbane': (27.4698, 153.0251),\n",
    "            'Perth': (31.9505, 115.8605),\n",
    "            'Adelaide': (34.9285, 138.6007),\n",
    "            'Canberra': (35.2809, 149.1300),\n",
    "            'Gold Coast': (28.0167, 153.4000),\n",
    "            'Newcastle': (32.9267, 151.7789),\n",
    "            'Auckland': (36.8485, 174.7633),\n",
    "            'Wellington': (41.2865, 174.7762),\n",
    "            'Christchurch': (43.5321, 172.6362),\n",
    "            \n",
    "            # AFRICA\n",
    "            'Lagos': (6.5244, 3.3792),\n",
    "            'Abuja': (9.0765, 7.3986),\n",
    "            'Kano': (12.0022, 8.5920),\n",
    "            'Ibadan': (7.3775, 3.9470),\n",
    "            'Cairo': (30.0444, 31.2357),\n",
    "            'Alexandria': (31.2001, 29.9187),\n",
    "            'Cape Town': (33.9249, 18.4241),\n",
    "            'Johannesburg': (26.2041, 28.0473),\n",
    "            'Durban': (29.8587, 31.0218),\n",
    "            'Nairobi': (1.2921, 36.8219),\n",
    "            'Addis Ababa': (9.1450, 38.7451),\n",
    "            'Casablanca': (33.5731, 7.5898),\n",
    "            'Tunis': (36.8065, 10.1815),\n",
    "            'Algiers': (36.7538, 3.0588),\n",
    "            \n",
    "            # SOUTH AMERICA\n",
    "            'S√£o Paulo': (23.5558, 46.6396),\n",
    "            'Rio de Janeiro': (22.9068, 43.1729),\n",
    "            'Bras√≠lia': (15.8267, 47.9218),\n",
    "            'Salvador': (12.9714, 38.5014),\n",
    "            'Buenos Aires': (34.6118, 58.3960),\n",
    "            'C√≥rdoba': (31.4201, 64.1888),\n",
    "            'Lima': (12.0464, 77.0428),\n",
    "            'Bogot√°': (4.7110, 74.0721),\n",
    "            'Medell√≠n': (6.2486, 75.5636),\n",
    "            'Caracas': (10.4806, 66.9036),\n",
    "            'Santiago': (33.4489, 70.6693),\n",
    "            'Quito': (0.1807, 78.4678),\n",
    "            'La Paz': (16.5000, 68.1193),\n",
    "            'Montevideo': (34.9011, 56.1645),\n",
    "            \n",
    "            # HIGH-RISK CYBERCRIME LOCATIONS\n",
    "            'Moscow': (55.7558, 37.6176),\n",
    "            'St. Petersburg': (59.9311, 30.3609),\n",
    "            'Novosibirsk': (55.0084, 82.9357),\n",
    "            'Yekaterinburg': (56.8431, 60.6454),\n",
    "            'Beijing': (39.9042, 116.4074),\n",
    "            'Shanghai': (31.2304, 121.4737),\n",
    "            'Shenzhen': (22.5431, 114.0579),\n",
    "            'Guangzhou': (23.1291, 113.2644),\n",
    "            'Hangzhou': (30.2741, 120.1551),\n",
    "            'Chengdu': (30.5728, 104.0668),\n",
    "            'Pyongyang': (39.0392, 125.7625),\n",
    "            'Hamhung': (39.9187, 127.5358),\n",
    "            'Chongjin': (41.7847, 129.7755),\n",
    "            'Minsk': (53.9006, 27.5590),\n",
    "            'Kiev': (50.4501, 30.5234),\n",
    "            'Kharkiv': (49.9935, 36.2304),\n",
    "            'Chisinau': (47.0105, 28.8638),\n",
    "            'Tirana': (41.3275, 19.8187),\n",
    "            'Skopje': (41.9973, 21.4280),\n",
    "            'Sarajevo': (43.8486, 18.3564),\n",
    "            \n",
    "            # ADDITIONAL ASIAN CITIES\n",
    "            'Mumbai': (19.0760, 72.8777),\n",
    "            'Delhi': (28.7041, 77.1025),\n",
    "            'Bangalore': (12.9716, 77.5946),\n",
    "            'Chennai': (13.0827, 80.2707),\n",
    "            'Hyderabad': (17.3850, 78.4867),\n",
    "            'Kolkata': (22.5726, 88.3639),\n",
    "            'Pune': (18.5204, 73.8567),\n",
    "            'Ahmedabad': (23.0225, 72.5714),\n",
    "            'Karachi': (24.8607, 67.0011),\n",
    "            'Lahore': (31.5204, 74.3587),\n",
    "            'Islamabad': (33.7294, 73.0931),\n",
    "            'Kabul': (34.5553, 69.2075),\n",
    "            'Tashkent': (41.2995, 69.2401),\n",
    "            'Almaty': (43.2220, 76.8512),\n",
    "            'Bishkek': (42.8746, 74.5698),\n",
    "            'Dushanbe': (38.5598, 68.7870),\n",
    "        }\n",
    "    \n",
    "    def _define_travel_zones(self):\n",
    "        \"\"\"\n",
    "        Define comprehensive risk zones for different types of travel destinations\n",
    "        Based on Filipino travel patterns, OFW destinations, and threat intelligence\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'ofw_hubs': {\n",
    "                'locations': [\n",
    "                    # Middle East - Major OFW employment destinations\n",
    "                    'Dubai', 'Abu Dhabi', 'Sharjah', 'Ajman', 'Al Ain',\n",
    "                    'Riyadh', 'Jeddah', 'Dammam', 'Mecca', 'Medina',\n",
    "                    'Doha', 'Kuwait City', 'Manama', 'Muscat'\n",
    "                ],\n",
    "                'base_risk': 0.2,  # Low risk for legitimate OFW destinations\n",
    "                'description': 'Major OFW employment hubs in Middle East'\n",
    "            },\n",
    "            'business_hubs': {\n",
    "                'locations': [\n",
    "                    # Asia Pacific business centers\n",
    "                    'Singapore', 'Hong Kong', 'Macau',\n",
    "                    'Tokyo', 'Osaka', 'Nagoya', 'Kyoto', 'Yokohama',\n",
    "                    'Seoul', 'Busan', 'Incheon',\n",
    "                    'Bangkok', 'Phuket', 'Kuala Lumpur', 'Penang',\n",
    "                    'Jakarta', 'Bali', 'Surabaya',\n",
    "                    # Global financial centers\n",
    "                    'London', 'Frankfurt', 'Zurich', 'Geneva',\n",
    "                    'New York', 'Chicago', 'Boston', 'San Francisco'\n",
    "                ],\n",
    "                'base_risk': 0.25,\n",
    "                'description': 'Regional and global business centers'\n",
    "            },\n",
    "            'diaspora_hubs': {\n",
    "                'locations': [\n",
    "                    # United States - Large Filipino communities\n",
    "                    'Los Angeles', 'San Francisco', 'San Diego', 'San Jose',\n",
    "                    'Las Vegas', 'Seattle', 'New York', 'Jersey City',\n",
    "                    'Chicago', 'Houston', 'Miami', 'Honolulu',\n",
    "                    # Canada - Filipino diaspora\n",
    "                    'Toronto', 'Vancouver', 'Montreal', 'Calgary', 'Edmonton',\n",
    "                    # Australia/New Zealand - Filipino communities\n",
    "                    'Sydney', 'Melbourne', 'Brisbane', 'Perth', 'Adelaide',\n",
    "                    'Auckland', 'Wellington'\n",
    "                ],\n",
    "                'base_risk': 0.3,\n",
    "                'description': 'Major Filipino diaspora communities'\n",
    "            },\n",
    "            'tourism_destinations': {\n",
    "                'locations': [\n",
    "                    # Europe - Common tourist destinations\n",
    "                    'Paris', 'Lyon', 'Rome', 'Milan', 'Venice', 'Naples',\n",
    "                    'Madrid', 'Barcelona', 'Berlin', 'Munich',\n",
    "                    'Amsterdam', 'Brussels', 'Vienna',\n",
    "                    'Stockholm', 'Oslo', 'Copenhagen', 'Helsinki',\n",
    "                    'Prague', 'Budapest', 'Warsaw', 'Athens',\n",
    "                    'Istanbul', 'Dublin', 'Edinburgh',\n",
    "                    # Other popular destinations\n",
    "                    'Ho Chi Minh City', 'Hanoi', 'Da Nang',\n",
    "                    'Phnom Penh', 'Vientiane', 'Yangon',\n",
    "                    'Mumbai', 'Delhi', 'Bangalore'\n",
    "                ],\n",
    "                'base_risk': 0.35,\n",
    "                'description': 'Popular tourist and cultural destinations'\n",
    "            },\n",
    "            'developing_markets': {\n",
    "                'locations': [\n",
    "                    # South/Southeast Asia\n",
    "                    'Chennai', 'Hyderabad', 'Kolkata', 'Pune', 'Ahmedabad',\n",
    "                    'Karachi', 'Lahore', 'Islamabad', 'Dhaka',\n",
    "                    'Colombo', 'Kathmandu', 'Kabul',\n",
    "                    # Central Asia\n",
    "                    'Tashkent', 'Almaty', 'Bishkek', 'Dushanbe',\n",
    "                    # Africa\n",
    "                    'Cairo', 'Alexandria', 'Cape Town', 'Johannesburg',\n",
    "                    'Nairobi', 'Addis Ababa', 'Casablanca',\n",
    "                    # South America\n",
    "                    'S√£o Paulo', 'Rio de Janeiro', 'Buenos Aires', 'Lima'\n",
    "                ],\n",
    "                'base_risk': 0.45,\n",
    "                'description': 'Developing markets with moderate risk'\n",
    "            },\n",
    "            'high_risk_regions': {\n",
    "                'locations': [\n",
    "                    # Africa - Higher risk areas\n",
    "                    'Lagos', 'Abuja', 'Kano', 'Ibadan',\n",
    "                    'Tunis', 'Algiers',\n",
    "                    # South America - Crime hotspots\n",
    "                    'Bogot√°', 'Medell√≠n', 'Caracas', 'La Paz',\n",
    "                    # Conflict zones\n",
    "                    'Baghdad', 'Beirut', 'Amman'\n",
    "                ],\n",
    "                'base_risk': 0.65,\n",
    "                'description': 'Higher risk regions with security concerns'\n",
    "            },\n",
    "            'cybercrime_hubs': {\n",
    "                'locations': [\n",
    "                    # Russia - Major cybercrime source\n",
    "                    'Moscow', 'St. Petersburg', 'Novosibirsk', 'Yekaterinburg',\n",
    "                    # China - State-sponsored threats\n",
    "                    'Beijing', 'Shanghai', 'Shenzhen', 'Guangzhou', \n",
    "                    'Hangzhou', 'Chengdu',\n",
    "                    # North Korea - State actors\n",
    "                    'Pyongyang', 'Hamhung', 'Chongjin',\n",
    "                    # Iran - Cyber warfare\n",
    "                    'Tehran', 'Isfahan', 'Mashhad',\n",
    "                    # Eastern Europe - Cybercrime centers\n",
    "                    'Bucharest', 'Minsk', 'Kiev', 'Kharkiv',\n",
    "                    'Chisinau', 'Tirana', 'Skopje', 'Sarajevo'\n",
    "                ],\n",
    "                'base_risk': 0.8,\n",
    "                'description': 'Known cybercrime and state-sponsored threat locations'\n",
    "            },\n",
    "            'philippines_domestic': {\n",
    "                'locations': [\n",
    "                    # Metro Manila\n",
    "                    'Manila', 'Quezon City', 'Makati', 'Taguig', 'Pasig',\n",
    "                    'Mandaluyong', 'Marikina', 'Pasay', 'Para√±aque',\n",
    "                    'Las Pi√±as', 'Muntinlupa', 'Caloocan', 'Valenzuela',\n",
    "                    'Malabon', 'Navotas',\n",
    "                    # Luzon\n",
    "                    'Baguio', 'Angeles', 'San Fernando', 'Dagupan',\n",
    "                    'Cabanatuan', 'Olongapo', 'Batangas', 'Lipa',\n",
    "                    'Lucena', 'Naga', 'Legazpi', 'Vigan', 'Tuguegarao', 'Laoag',\n",
    "                    # Visayas\n",
    "                    'Cebu City', 'Mandaue', 'Lapu-Lapu', 'Iloilo City',\n",
    "                    'Bacolod', 'Dumaguete', 'Tacloban', 'Ormoc',\n",
    "                    'Tagbilaran', 'Roxas', 'Kalibo',\n",
    "                    # Mindanao\n",
    "                    'Davao City', 'Cagayan de Oro', 'Zamboanga', 'Butuan',\n",
    "                    'Iligan', 'Cotabato', 'General Santos', 'Koronadal',\n",
    "                    'Kidapawan', 'Dipolog', 'Pagadian', 'Marawi'\n",
    "                ],\n",
    "                'base_risk': 0.05,  # Very low risk for domestic access\n",
    "                'description': 'Philippine domestic locations'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_user_baseline(self, user_login_history):\n",
    "        \"\"\"\n",
    "        Analyze user's normal behavior patterns from login history\n",
    "        \n",
    "        Args:\n",
    "            user_login_history: List of dictionaries with login data\n",
    "        \"\"\"\n",
    "        user_id = user_login_history[0]['user_id']\n",
    "        \n",
    "        # Extract behavioral patterns\n",
    "        locations = [login['location'] for login in user_login_history]\n",
    "        times = [datetime.strptime(login['timestamp'], '%Y-%m-%d %H:%M:%S') for login in user_login_history]\n",
    "        devices = [login['device_type'] for login in user_login_history]\n",
    "        countries = [login['country'] for login in user_login_history]\n",
    "        \n",
    "        # Calculate baseline patterns\n",
    "        baseline = {\n",
    "            'user_id': user_id,\n",
    "            'home_locations': list(set(loc for loc, country in zip(locations, countries) if country == 'PH')),\n",
    "            'common_devices': list(set(devices)),\n",
    "            'typical_hours': [t.hour for t in times],\n",
    "            'login_frequency': len(user_login_history),\n",
    "            'countries_visited': list(set(countries)),\n",
    "            'last_known_location': locations[-1],\n",
    "            'last_login_time': times[-1],\n",
    "            'travel_history': self._extract_travel_history(user_login_history)\n",
    "        }\n",
    "        \n",
    "        # Store user profile\n",
    "        self.user_profiles[user_id] = baseline\n",
    "        \n",
    "        print(f\"‚úÖ Baseline established for User {user_id}\")\n",
    "        print(f\"   Home locations: {baseline['home_locations']}\")\n",
    "        print(f\"   Countries visited: {baseline['countries_visited']}\")\n",
    "        print(f\"   Common devices: {baseline['common_devices']}\")\n",
    "        \n",
    "        return baseline\n",
    "    \n",
    "    def _extract_travel_history(self, login_history):\n",
    "        \"\"\"\n",
    "        Extract travel patterns from login history\n",
    "        \"\"\"\n",
    "        travels = []\n",
    "        \n",
    "        for i in range(1, len(login_history)):\n",
    "            prev_login = login_history[i-1]\n",
    "            curr_login = login_history[i]\n",
    "            \n",
    "            if prev_login['country'] != curr_login['country']:\n",
    "                travel = {\n",
    "                    'from_location': prev_login['location'],\n",
    "                    'to_location': curr_login['location'],\n",
    "                    'from_country': prev_login['country'],\n",
    "                    'to_country': curr_login['country'],\n",
    "                    'time_gap': (datetime.strptime(curr_login['timestamp'], '%Y-%m-%d %H:%M:%S') - \n",
    "                               datetime.strptime(prev_login['timestamp'], '%Y-%m-%d %H:%M:%S')).total_seconds() / 3600,\n",
    "                    'distance_km': self._calculate_distance(prev_login['location'], curr_login['location'])\n",
    "                }\n",
    "                travels.append(travel)\n",
    "        \n",
    "        return travels\n",
    "    \n",
    "\n",
    "    @lru_cache(maxsize=1000)\n",
    "    def _calculate_distance(self, location1, location2):\n",
    "        \"\"\"\n",
    "        Calculate distance between two locations\n",
    "        \"\"\"\n",
    "        if location1 in self.location_coordinates and location2 in self.location_coordinates:\n",
    "            coord1 = self.location_coordinates[location1]\n",
    "            coord2 = self.location_coordinates[location2]\n",
    "            return geodesic(coord1, coord2).kilometers\n",
    "        return 0\n",
    "    \n",
    "    def _get_location_zone(self, location):\n",
    "        \"\"\"\n",
    "        Determine which risk zone a location belongs to\n",
    "        \"\"\"\n",
    "        for zone_name, zone_data in self.travel_risk_zones.items():\n",
    "            if location in zone_data['locations']:\n",
    "                return zone_name, zone_data['base_risk']\n",
    "        return 'unknown', 0.5  # Default for unknown locations\n",
    "    \n",
    "    def analyze_travel_plausibility(self, user_id, new_login):\n",
    "        \"\"\"\n",
    "        Analyze if travel to new location is physically plausible\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return {\n",
    "                'plausible': False,\n",
    "                'reason': 'No user baseline established',\n",
    "                'risk_modifier': 0.5\n",
    "            }\n",
    "        \n",
    "        profile = self.user_profiles[user_id]\n",
    "        last_location = profile['last_known_location']\n",
    "        last_time = profile['last_login_time']\n",
    "        \n",
    "        new_location = new_login['location']\n",
    "        new_time = datetime.strptime(new_login['timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Calculate travel requirements\n",
    "        distance_km = self._calculate_distance(last_location, new_location)\n",
    "        time_gap_hours = (new_time - last_time).total_seconds() / 3600\n",
    "        \n",
    "        if distance_km == 0:  # Same location or unknown coordinates\n",
    "            return {\n",
    "                'plausible': True,\n",
    "                'reason': 'Same location or local area',\n",
    "                'risk_modifier': 0.0,\n",
    "                'distance_km': distance_km,\n",
    "                'time_gap_hours': time_gap_hours\n",
    "            }\n",
    "        \n",
    "        # Calculate minimum travel time (assuming commercial flight)\n",
    "        min_travel_time_hours = distance_km / self.max_travel_speed_kmh\n",
    "        buffer_time_hours = 4  # Airport procedures, layovers, etc.\n",
    "        required_time_hours = min_travel_time_hours + buffer_time_hours\n",
    "        \n",
    "        # Check plausibility\n",
    "        if time_gap_hours >= required_time_hours:\n",
    "            return {\n",
    "                'plausible': True,\n",
    "                'reason': f'Sufficient time for travel ({time_gap_hours:.1f}h vs {required_time_hours:.1f}h required)',\n",
    "                'risk_modifier': 0.0,\n",
    "                'distance_km': distance_km,\n",
    "                'time_gap_hours': time_gap_hours,\n",
    "                'required_time_hours': required_time_hours\n",
    "            }\n",
    "        else:\n",
    "            # Impossible travel\n",
    "            return {\n",
    "                'plausible': False,\n",
    "                'reason': f'Impossible travel: {distance_km:.0f}km in {time_gap_hours:.1f}h (need {required_time_hours:.1f}h)',\n",
    "                'risk_modifier': 0.8,  # Very high risk for impossible travel\n",
    "                'distance_km': distance_km,\n",
    "                'time_gap_hours': time_gap_hours,\n",
    "                'required_time_hours': required_time_hours\n",
    "            }\n",
    "    \n",
    "    def analyze_behavioral_consistency(self, user_id, new_login):\n",
    "        \"\"\"\n",
    "        Check if user behavior remains consistent despite location change\n",
    "        \"\"\"\n",
    "        if user_id not in self.user_profiles:\n",
    "            return {'consistency_score': 0.5, 'factors': ['No baseline']}\n",
    "        \n",
    "        profile = self.user_profiles[user_id]\n",
    "        consistency_factors = []\n",
    "        consistency_score = 1.0  # Start with perfect consistency\n",
    "        \n",
    "        # Device consistency\n",
    "        if new_login['device_type'] in profile['common_devices']:\n",
    "            consistency_factors.append('Known device type')\n",
    "        else:\n",
    "            consistency_score -= 0.3\n",
    "            consistency_factors.append('New device type')\n",
    "        \n",
    "        # Time pattern consistency (adjusted for timezone)\n",
    "        new_hour = datetime.strptime(new_login['timestamp'], '%Y-%m-%d %H:%M:%S').hour\n",
    "        if new_hour in profile['typical_hours'] or abs(new_hour - np.mean(profile['typical_hours'])) <= 3:\n",
    "            consistency_factors.append('Consistent login time')\n",
    "        else:\n",
    "            consistency_score -= 0.2\n",
    "            consistency_factors.append('Unusual login time')\n",
    "        \n",
    "        # Previous travel history\n",
    "        if new_login['country'] in profile['countries_visited']:\n",
    "            consistency_factors.append('Previously visited country')\n",
    "            consistency_score += 0.1  # Bonus for familiar destinations\n",
    "        else:\n",
    "            consistency_factors.append('First visit to country')\n",
    "        \n",
    "        # Browser/session consistency (if available)\n",
    "        # This would check if session tokens, browser fingerprints match\n",
    "        # For demo, we'll simulate this\n",
    "        if np.random.random() > 0.3:  # 70% chance of consistent browser\n",
    "            consistency_factors.append('Consistent browser fingerprint')\n",
    "        else:\n",
    "            consistency_score -= 0.2\n",
    "            consistency_factors.append('Different browser fingerprint')\n",
    "        \n",
    "        return {\n",
    "            'consistency_score': max(0, min(1, consistency_score)),\n",
    "            'factors': consistency_factors\n",
    "        }\n",
    "    \n",
    "    def calculate_travel_aware_risk(self, user_id, new_login):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive travel-aware risk score\n",
    "        \"\"\"\n",
    "        # Base risk components\n",
    "        risk_components = {}\n",
    "        \n",
    "        # 1. Location zone risk\n",
    "        zone, base_location_risk = self._get_location_zone(new_login['location'])\n",
    "        risk_components['location_zone'] = base_location_risk\n",
    "        \n",
    "        # 2. Travel plausibility\n",
    "        travel_analysis = self.analyze_travel_plausibility(user_id, new_login)\n",
    "        risk_components['travel_plausibility'] = travel_analysis['risk_modifier']\n",
    "        \n",
    "        # 3. Behavioral consistency\n",
    "        behavior_analysis = self.analyze_behavioral_consistency(user_id, new_login)\n",
    "        behavior_risk = 1 - behavior_analysis['consistency_score']\n",
    "        risk_components['behavioral_inconsistency'] = behavior_risk\n",
    "        \n",
    "        # 4. Technical indicators\n",
    "        technical_risk = 0.0\n",
    "        technical_factors = []\n",
    "        \n",
    "        if new_login.get('is_attack_ip', False):\n",
    "            technical_risk += 0.4\n",
    "            technical_factors.append('Known attack IP')\n",
    "        \n",
    "        if new_login.get('high_latency', False):\n",
    "            technical_risk += 0.2\n",
    "            technical_factors.append('High network latency')\n",
    "        \n",
    "        if not new_login.get('login_successful', True):\n",
    "            technical_risk += 0.3\n",
    "            technical_factors.append('Failed login attempt')\n",
    "        \n",
    "        risk_components['technical_indicators'] = technical_risk\n",
    "        \n",
    "        # 5. Geographic distance factor\n",
    "        if user_id in self.user_profiles:\n",
    "            distance_km = self._calculate_distance(\n",
    "                self.user_profiles[user_id]['last_known_location'],\n",
    "                new_login['location']\n",
    "            )\n",
    "            # Risk increases with distance, but caps at 0.3\n",
    "            distance_risk = min(0.3, distance_km / 10000)  # 10,000km = max distance risk\n",
    "            risk_components['distance'] = distance_risk\n",
    "        else:\n",
    "            risk_components['distance'] = 0.2\n",
    "        \n",
    "        # Calculate weighted final risk score\n",
    "        weights = {\n",
    "            'location_zone': 0.3,\n",
    "            'travel_plausibility': 0.25,\n",
    "            'behavioral_inconsistency': 0.2,\n",
    "            'technical_indicators': 0.15,\n",
    "            'distance': 0.1\n",
    "        }\n",
    "        \n",
    "        final_risk = sum(risk_components[component] * weights[component] \n",
    "                        for component in risk_components)\n",
    "        \n",
    "        # Cap at 1.0\n",
    "        final_risk = min(1.0, final_risk)\n",
    "        \n",
    "        return {\n",
    "            'final_risk_score': final_risk,\n",
    "            'risk_components': risk_components,\n",
    "            'travel_analysis': travel_analysis,\n",
    "            'behavior_analysis': behavior_analysis,\n",
    "            'location_zone': zone,\n",
    "            'technical_factors': technical_factors\n",
    "        }\n",
    "    \n",
    "    def generate_travel_aware_explanation(self, risk_analysis, language='english'):\n",
    "        \"\"\"\n",
    "        Generate human-readable explanation for the risk decision\n",
    "        \"\"\"\n",
    "        risk_score = risk_analysis['final_risk_score']\n",
    "        travel_info = risk_analysis['travel_analysis']\n",
    "        behavior_info = risk_analysis['behavior_analysis']\n",
    "        \n",
    "        # Determine risk level\n",
    "        if risk_score < 0.3:\n",
    "            risk_level = \"LOW\"\n",
    "            action = \"ALLOW\"\n",
    "            message_key = \"low_risk\"\n",
    "        elif risk_score < 0.6:\n",
    "            risk_level = \"MEDIUM\"\n",
    "            action = \"ALLOW_WITH_OTP\"\n",
    "            message_key = \"medium_risk\"\n",
    "        else:\n",
    "            risk_level = \"HIGH\"\n",
    "            action = \"BLOCK\" if not travel_info['plausible'] else \"STRICT_VERIFICATION\"\n",
    "            message_key = \"high_risk\"\n",
    "        \n",
    "        # Build explanation factors\n",
    "        explanation_factors = []\n",
    "        \n",
    "        # Travel plausibility\n",
    "        if travel_info['plausible']:\n",
    "            explanation_factors.append(f\"‚úÖ Travel is plausible ({travel_info['reason']})\")\n",
    "        else:\n",
    "            explanation_factors.append(f\"‚ùå {travel_info['reason']}\")\n",
    "        \n",
    "        # Behavioral consistency\n",
    "        consistency_pct = behavior_info['consistency_score'] * 100\n",
    "        explanation_factors.append(f\"üîç Behavior consistency: {consistency_pct:.0f}%\")\n",
    "        \n",
    "        # Location zone\n",
    "        zone = risk_analysis['location_zone']\n",
    "        zone_info = self.travel_risk_zones.get(zone, {})\n",
    "        explanation_factors.append(f\"üìç Location: {zone_info.get('description', 'Unknown zone')}\")\n",
    "        \n",
    "        # Technical factors\n",
    "        if risk_analysis['technical_factors']:\n",
    "            explanation_factors.extend([f\"‚ö†Ô∏è {factor}\" for factor in risk_analysis['technical_factors']])\n",
    "        \n",
    "        return {\n",
    "            'risk_score': risk_score,\n",
    "            'risk_level': risk_level,\n",
    "            'action': action,\n",
    "            'explanation_factors': explanation_factors,\n",
    "            'travel_plausible': travel_info['plausible'],\n",
    "            'behavior_consistent': behavior_info['consistency_score'] > 0.7,\n",
    "            'recommendation': self._get_recommendation(risk_score, travel_info, behavior_info)\n",
    "        }\n",
    "    \n",
    "    def _get_recommendation(self, risk_score, travel_info, behavior_info):\n",
    "        \"\"\"\n",
    "        Generate specific recommendations based on analysis\n",
    "        \"\"\"\n",
    "        if not travel_info['plausible']:\n",
    "            return \"BLOCK: Impossible travel detected. Manual review required.\"\n",
    "        \n",
    "        if risk_score < 0.3 and behavior_info['consistency_score'] > 0.8:\n",
    "            return \"ALLOW: Legitimate travel with consistent behavior.\"\n",
    "        \n",
    "        if risk_score < 0.6 and travel_info['plausible']:\n",
    "            return \"ALLOW with SMS OTP: Possible legitimate travel, verify with additional authentication.\"\n",
    "        \n",
    "        return \"STRICT VERIFICATION: High-risk login requiring manual review and multiple authentication factors.\"\n",
    "\n",
    "def demo_travel_scenarios():\n",
    "    \"\"\"\n",
    "    Demonstrate travel-aware RBA with realistic scenarios\n",
    "    \"\"\"\n",
    "    print(\"üõ´ BANTAI TRAVEL-AWARE RBA DEMO\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize the travel-aware system\n",
    "    bantai_travel = BantAI_TravelAware()\n",
    "    \n",
    "    # Sample user: Juan, an OFW from Manila\n",
    "    user_id = \"juan_dela_cruz_123\"\n",
    "    \n",
    "    # Juan's login history (establishing baseline)\n",
    "    juan_history = [\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-01 09:00:00',\n",
    "            'location': 'Manila',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-03 14:30:00',\n",
    "            'location': 'Makati',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-05 11:15:00',\n",
    "            'location': 'Quezon City',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'desktop',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-08 16:45:00',\n",
    "            'location': 'Manila',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        },\n",
    "        {\n",
    "            'user_id': user_id,\n",
    "            'timestamp': '2024-01-10 08:20:00',\n",
    "            'location': 'Manila',\n",
    "            'country': 'PH',\n",
    "            'device_type': 'mobile',\n",
    "            'login_successful': True\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Establish baseline\n",
    "    print(\"\\nüìä ESTABLISHING USER BASELINE...\")\n",
    "    baseline = bantai_travel.analyze_user_baseline(juan_history)\n",
    "    \n",
    "    # Test scenarios\n",
    "    test_scenarios = [\n",
    "        {\n",
    "            'name': '‚úàÔ∏è Legitimate OFW Travel to Dubai',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-01-20 19:30:00',  # 10 days later, enough time to travel\n",
    "                'location': 'Dubai',\n",
    "                'country': 'AE',\n",
    "                'device_type': 'mobile',  # Same device\n",
    "                'login_successful': True,\n",
    "                'is_attack_ip': False,\n",
    "                'high_latency': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'üö® Impossible Travel Attack',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-01-20 20:30:00',  # 1 hour after Dubai login\n",
    "                'location': 'Moscow',\n",
    "                'country': 'RU',\n",
    "                'device_type': 'desktop',  # Different device\n",
    "                'login_successful': False,\n",
    "                'is_attack_ip': True,\n",
    "                'high_latency': True\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'üõÇ Business Trip to Singapore',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-01-25 10:00:00',  # 5 days later from Dubai\n",
    "                'location': 'Singapore',\n",
    "                'country': 'SG',\n",
    "                'device_type': 'mobile',  # Same device\n",
    "                'login_successful': True,\n",
    "                'is_attack_ip': False,\n",
    "                'high_latency': False\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'üë®‚Äçüíº Return to Philippines',\n",
    "            'login': {\n",
    "                'user_id': user_id,\n",
    "                'timestamp': '2024-02-01 15:00:00',  # 1 week later\n",
    "                'location': 'Manila',\n",
    "                'country': 'PH',\n",
    "                'device_type': 'mobile',\n",
    "                'login_successful': True,\n",
    "                'is_attack_ip': False,\n",
    "                'high_latency': False\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Analyze each scenario\n",
    "    print(f\"\\nüîç ANALYZING TRAVEL SCENARIOS...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for scenario in test_scenarios:\n",
    "        print(f\"\\n{scenario['name']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Calculate risk\n",
    "        risk_analysis = bantai_travel.calculate_travel_aware_risk(\n",
    "            user_id, scenario['login']\n",
    "        )\n",
    "        \n",
    "        # Generate explanation\n",
    "        explanation = bantai_travel.generate_travel_aware_explanation(risk_analysis)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"üìç Location: {scenario['login']['location']}, {scenario['login']['country']}\")\n",
    "        print(f\"üéØ Risk Score: {explanation['risk_score']:.2f} ({explanation['risk_level']})\")\n",
    "        print(f\"‚ö° Action: {explanation['action']}\")\n",
    "        print(f\"‚úÖ Travel Plausible: {explanation['travel_plausible']}\")\n",
    "        print(f\"üîÑ Behavior Consistent: {explanation['behavior_consistent']}\")\n",
    "        print(f\"üí° Recommendation: {explanation['recommendation']}\")\n",
    "        \n",
    "        print(\"\\nüîç Detailed Analysis:\")\n",
    "        for factor in explanation['explanation_factors']:\n",
    "            print(f\"   {factor}\")\n",
    "    \n",
    "    return bantai_travel\n",
    "\n",
    "#Unified API Endpoint\n",
    "def unified_risk_assessment(self, user_id, login_data):\n",
    "    # Combine ML + Travel systems\n",
    "    ml_risk = self.ml_component.predict(login_data)\n",
    "    travel_risk = self.travel_component.calculate_risk(user_id, login_data) \n",
    "    return weighted_average([ml_risk, travel_risk], [0.4, 0.3])\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the travel-aware demo\n",
    "    travel_system = demo_travel_scenarios()\n",
    "    \n",
    "    print(\"\\nüí° Usage for Custom Scenarios:\")\n",
    "    print(\"# 1. Establish user baseline:\")\n",
    "    print(\"baseline = travel_system.analyze_user_baseline(user_login_history)\")\n",
    "    print(\"\\n# 2. Analyze new login:\")\n",
    "    print(\"risk_analysis = travel_system.calculate_travel_aware_risk(user_id, new_login)\")\n",
    "    print(\"explanation = travel_system.generate_travel_aware_explanation(risk_analysis)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BantAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
