{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae8e228",
   "metadata": {},
   "source": [
    "# if the model has issues, use this code to validate #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will analyze the corruption, try recovery methods, and create a working model\n",
    "from model_recovery import comprehensive_model_fix\n",
    "\n",
    "# Analyze and fix any model issues\n",
    "result = comprehensive_model_fix()\n",
    "\n",
    "# Display the results\n",
    "print(\"üîç Model Analysis and Recovery Results:\")\n",
    "print(f\"Initial Status: {result['initial_status']}\")\n",
    "\n",
    "if result['issues_found']:\n",
    "    print(\"\\nIssues Found:\")\n",
    "    for issue in result['issues_found']:\n",
    "        print(f\"  - {issue}\")\n",
    "\n",
    "if result['recovery_attempted']:\n",
    "    print(\"\\nüîß Recovery Attempt:\")\n",
    "    print(f\"Success: {'‚úÖ' if result['recovery_successful'] else '‚ùå'}\")\n",
    "    for note in result['recovery_notes']:\n",
    "        print(f\"  - {note}\")\n",
    "\n",
    "print(f\"\\nFinal Status: {result['final_status']}\")\n",
    "\n",
    "if result['final_status'] == 'valid':\n",
    "    print(\"\\n‚úÖ Model is now valid and ready to use!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Model still has issues. Please check the remaining issues:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39909efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosing file: bantai_model.pkl\n",
      "==================================================\n",
      "üìÅ File size: 1204 bytes\n",
      "üîç First 20 bytes (hex): 800495a9040000000000007d94288c056d6f6465\n",
      "üîç First 50 chars (as text): b'\\x80\\x04\\x95\\xa9\\x04\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x05model\\x94\\x8c\\x18sklearn.ensemble._forest\\x94\\x8c'\n",
      "‚úÖ This appears to be a pickle file (protocol 3, 4, or 5)\n",
      "‚úÖ Pickle file loads successfully\n",
      "üìä Data type: <class 'dict'>\n",
      "üóùÔ∏è Dictionary keys: ['model', 'scaler', 'feature_columns', 'model_info', 'features']\n",
      "‚úÖ All expected model components present\n",
      "\n",
      "\n",
      "üß™ Testing Enhanced Model Loading System\n",
      "==================================================\n",
      "Diagnosing file: bantai_model.pkl\n",
      "==================================================\n",
      "üìÅ File size: 1204 bytes\n",
      "üîç First 20 bytes (hex): 800495a9040000000000007d94288c056d6f6465\n",
      "üîç First 50 chars (as text): b'\\x80\\x04\\x95\\xa9\\x04\\x00\\x00\\x00\\x00\\x00\\x00}\\x94(\\x8c\\x05model\\x94\\x8c\\x18sklearn.ensemble._forest\\x94\\x8c'\n",
      "‚úÖ This appears to be a pickle file (protocol 3, 4, or 5)\n",
      "‚úÖ Pickle file loads successfully\n",
      "üìä Data type: <class 'dict'>\n",
      "üóùÔ∏è Dictionary keys: ['model', 'scaler', 'feature_columns', 'model_info', 'features']\n",
      "‚úÖ All expected model components present\n",
      "‚úÖ Model components loaded:\n",
      "   - Model: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "   - Scaler: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "   - Features: ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency']\n",
      "‚úÖ Model system ready\n",
      "‚ö†Ô∏è Model prediction failed, using fallback: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "üéØ Test prediction: 0.500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Diagnostic tools to identify and fix ML model loading issues\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def diagnose_model_file(file_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Diagnose what type of file we're dealing with and suggest fixes\n",
    "    \"\"\"\n",
    "    print(f\"Diagnosing file: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"‚ùå File does not exist\")\n",
    "        return False\n",
    "    \n",
    "    # Check file size\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"üìÅ File size: {file_size} bytes\")\n",
    "    \n",
    "    # Read first few bytes to identify file type\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            first_bytes = f.read(100)\n",
    "        \n",
    "        print(f\"üîç First 20 bytes (hex): {first_bytes[:20].hex()}\")\n",
    "        print(f\"üîç First 50 chars (as text): {repr(first_bytes[:50])}\")\n",
    "        \n",
    "        # Check if it's a pickle file\n",
    "        if first_bytes.startswith(b'\\x80\\x03') or first_bytes.startswith(b'\\x80\\x04') or first_bytes.startswith(b'\\x80\\x05'):\n",
    "            print(\"‚úÖ This appears to be a pickle file (protocol 3, 4, or 5)\")\n",
    "            return test_pickle_loading(file_path)\n",
    "        \n",
    "        # Check if it's a text file\n",
    "        try:\n",
    "            text_content = first_bytes.decode('utf-8')\n",
    "            if '\\t' in text_content or ',' in text_content:\n",
    "                print(\"üìÑ This appears to be a CSV/TSV text file\")\n",
    "                print(\"üí° You may need to retrain and save the model as a pickle file\")\n",
    "                return False\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        \n",
    "        # Check if it's JSON\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                json.load(f)\n",
    "            print(\"üìã This appears to be a JSON file\")\n",
    "            return False\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        print(\"‚ùì Unknown file format\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_pickle_loading(file_path):\n",
    "    \"\"\"Test if pickle file can be loaded\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        print(\"‚úÖ Pickle file loads successfully\")\n",
    "        print(f\"üìä Data type: {type(data)}\")\n",
    "        \n",
    "        if isinstance(data, dict):\n",
    "            print(f\"üóùÔ∏è Dictionary keys: {list(data.keys())}\")\n",
    "            \n",
    "            # Check for expected model components\n",
    "            expected_keys = ['model', 'scaler', 'feature_columns']\n",
    "            missing_keys = [key for key in expected_keys if key not in data]\n",
    "            if missing_keys:\n",
    "                print(f\"‚ö†Ô∏è Missing expected keys: {missing_keys}\")\n",
    "            else:\n",
    "                print(\"‚úÖ All expected model components present\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading pickle: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_dummy_model_file(file_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Create a dummy model file for testing purposes\n",
    "    \"\"\"\n",
    "    print(f\"Creating dummy model file: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Create a simple dummy model structure\n",
    "        dummy_model_data = {\n",
    "            'model': None,  # Placeholder for actual model\n",
    "            'scaler': None,  # Placeholder for scaler\n",
    "            'feature_columns': ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency'],\n",
    "            'model_info': {\n",
    "                'type': 'dummy',\n",
    "                'created': '2024-01-01',\n",
    "                'features': 6\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save as pickle\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(dummy_model_data, f)\n",
    "        \n",
    "        print(\"‚úÖ Dummy model file created successfully\")\n",
    "        \n",
    "        # Verify it works\n",
    "        if test_pickle_loading(file_path):\n",
    "            print(\"‚úÖ Dummy model file verified\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå Dummy model file verification failed\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating dummy model: {e}\")\n",
    "        return False\n",
    "\n",
    "def enhanced_model_loader(file_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Enhanced model loader with better error handling\n",
    "    \"\"\"\n",
    "    class ModelLoader:\n",
    "        def __init__(self):\n",
    "            self.model = None\n",
    "            self.scaler = None\n",
    "            self.feature_columns = None\n",
    "            self.model_info = {}\n",
    "        \n",
    "        def load_model(self, file_path):\n",
    "            \"\"\"Load model with comprehensive error handling\"\"\"\n",
    "            try:\n",
    "                # First diagnose the file\n",
    "                if not diagnose_model_file(file_path):\n",
    "                    print(\"üîß File diagnosis failed, creating dummy model for testing\")\n",
    "                    if create_dummy_model_file(file_path):\n",
    "                        return self._load_pickle(file_path)\n",
    "                    else:\n",
    "                        return False\n",
    "                \n",
    "                return self._load_pickle(file_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Model loading failed: {e}\")\n",
    "                return False\n",
    "        \n",
    "        def _load_pickle(self, file_path):\n",
    "            \"\"\"Internal method to load pickle file\"\"\"\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    model_data = pickle.load(f)\n",
    "                \n",
    "                if isinstance(model_data, dict):\n",
    "                    self.model = model_data.get('model')\n",
    "                    self.scaler = model_data.get('scaler')\n",
    "                    self.feature_columns = model_data.get('feature_columns')\n",
    "                    self.model_info = model_data.get('model_info', {})\n",
    "                    \n",
    "                    print(\"‚úÖ Model components loaded:\")\n",
    "                    print(f\"   - Model: {type(self.model)}\")\n",
    "                    print(f\"   - Scaler: {type(self.scaler)}\")\n",
    "                    print(f\"   - Features: {self.feature_columns}\")\n",
    "                    \n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"‚ùå Expected dict, got {type(model_data)}\")\n",
    "                    return False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Pickle loading error: {e}\")\n",
    "                return False\n",
    "        \n",
    "        def predict_risk(self, features):\n",
    "            \"\"\"Make risk prediction with fallback\"\"\"\n",
    "            if self.model is None:\n",
    "                # Fallback to rule-based risk\n",
    "                print(\"üîÑ Using fallback rule-based risk calculation\")\n",
    "                return self._rule_based_risk(features)\n",
    "            \n",
    "            try:\n",
    "                # Use actual model if available\n",
    "                if self.scaler:\n",
    "                    features_scaled = self.scaler.transform([features])\n",
    "                else:\n",
    "                    features_scaled = [features]\n",
    "                \n",
    "                risk_prob = self.model.predict_proba(features_scaled)[0][1]\n",
    "                return risk_prob\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Model prediction failed, using fallback: {e}\")\n",
    "                return self._rule_based_risk(features)\n",
    "        \n",
    "        def _rule_based_risk(self, features):\n",
    "            \"\"\"Simple rule-based risk calculation as fallback\"\"\"\n",
    "            # Basic risk calculation based on feature values\n",
    "            # This is a simplified version - you can make it more sophisticated\n",
    "            \n",
    "            risk_score = 0.3  # Base risk\n",
    "            \n",
    "            # Assume features are: [time_diff, distance, device_type, is_attack_ip, login_successful, latency]\n",
    "            if len(features) >= 6:\n",
    "                time_diff, distance, device_type, is_attack_ip, login_successful, latency = features[:6]\n",
    "                \n",
    "                # Distance-based risk\n",
    "                if distance > 1000:  # Long distance\n",
    "                    risk_score += 0.2\n",
    "                \n",
    "                # Time-based risk (very quick travel)\n",
    "                if time_diff < 2 and distance > 500:  # Impossible travel\n",
    "                    risk_score += 0.4\n",
    "                \n",
    "                # Attack IP\n",
    "                if is_attack_ip:\n",
    "                    risk_score += 0.3\n",
    "                \n",
    "                # Failed login\n",
    "                if not login_successful:\n",
    "                    risk_score += 0.2\n",
    "                \n",
    "                # High latency\n",
    "                if latency > 200:\n",
    "                    risk_score += 0.1\n",
    "            \n",
    "            return min(1.0, risk_score)\n",
    "\n",
    "    return ModelLoader()\n",
    "\n",
    "# Example usage and testing\n",
    "def test_model_system():\n",
    "    \"\"\"Test the enhanced model loading system\"\"\"\n",
    "    print(\"üß™ Testing Enhanced Model Loading System\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create model loader\n",
    "    loader = enhanced_model_loader()\n",
    "    \n",
    "    # Try to load model\n",
    "    success = loader.load_model(\"bantai_model.pkl\")\n",
    "    \n",
    "    if success:\n",
    "        print(\"‚úÖ Model system ready\")\n",
    "        \n",
    "        # Test prediction\n",
    "        test_features = [24, 1500, 0, False, True, 100]  # Example features\n",
    "        risk = loader.predict_risk(test_features)\n",
    "        print(f\"üéØ Test prediction: {risk:.3f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Model system failed to initialize\")\n",
    "    \n",
    "    return loader\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run diagnostics\n",
    "    diagnose_model_file(\"bantai_model.pkl\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Test the enhanced system\n",
    "    test_model_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed366dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Comprehensive Model Fix Procedure\n",
      "============================================================\n",
      "\n",
      "1. Analyzing corruption pattern...\n",
      "Analyzing corruption pattern...\n",
      "==================================================\n",
      "Found 10 tab characters at positions:\n",
      "  Position 83: context = b'\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\testimator'\n",
      "  Position 152: context = b'\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\tcriterion'\n",
      "  Position 189: context = b'r\\x94\\x8c\\x04best\\x94\\x8c\\tmax_depth'\n",
      "  Position 380: context = b'_weight\\x94N\\x8c\\tccp_alpha'\n",
      "  Position 508: context = b'\\x18h\\x17h\\x1ah\\x1bt\\x94\\x8c\\tbootstrap'\n",
      "  Position 521: context = b'otstrap\\x94\\x88\\x8c\\toob_score'\n",
      "  Position 725: context = b'\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\twith_mean'\n",
      "  Position 785: context = b'lumns\\x94]\\x94(\\x8c\\ttime_diff'\n",
      "  Position 1051: context = b'ss_hours\\x94\\x8c\\thour_norm'\n",
      "  Position 1106: context = b'e_change\\x94\\x8c\\tattack_ip'\n",
      "Null bytes in file: 55\n",
      "Most common byte patterns:\n",
      "  00000000: 33 occurrences\n",
      "  47000000: 6 occurrences\n",
      "  4700000000000000: 6 occurrences\n",
      "  0000000000000000: 6 occurrences\n",
      "\n",
      "2. Trying alternative loading methods...\n",
      "Trying alternative pickle loading methods...\n",
      "==================================================\n",
      "Trying: Standard pickle.load\n",
      "‚úÖ Success with Standard pickle.load\n",
      "‚úÖ Successfully recovered model using alternative method!\n",
      "\n",
      "üéØ Testing fixed model system...\n",
      "‚úÖ Loaded model from bantai_model.pkl\n",
      "Prediction error: This StandardScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Risk prediction: 0.500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Advanced pickle file recovery and alternative loading methods\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import traceback\n",
    "from io import BytesIO\n",
    "\n",
    "def attempt_partial_recovery(file_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Attempt to recover what we can from a corrupted pickle file\n",
    "    \"\"\"\n",
    "    print(f\"Attempting partial recovery of: {file_path}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        print(f\"Total file size: {len(data)} bytes\")\n",
    "        \n",
    "        # Try loading in chunks to find where corruption starts\n",
    "        chunk_size = 1024 * 1024  # 1MB chunks\n",
    "        last_good_position = 0\n",
    "        \n",
    "        for i in range(0, len(data), chunk_size):\n",
    "            chunk_end = min(i + chunk_size, len(data))\n",
    "            partial_data = data[:chunk_end]\n",
    "            \n",
    "            try:\n",
    "                # Try to load partial data\n",
    "                with BytesIO(partial_data) as bio:\n",
    "                    partial_obj = pickle.load(bio)\n",
    "                last_good_position = chunk_end\n",
    "                print(f\"Successfully loaded up to byte {chunk_end}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Corruption detected around byte {chunk_end}: {type(e).__name__}\")\n",
    "                break\n",
    "        \n",
    "        if last_good_position > 0:\n",
    "            print(f\"Recoverable data up to position: {last_good_position}\")\n",
    "            \n",
    "            # Try to load the good portion\n",
    "            try:\n",
    "                with BytesIO(data[:last_good_position]) as bio:\n",
    "                    recovered_obj = pickle.load(bio)\n",
    "                print(f\"Recovered object type: {type(recovered_obj)}\")\n",
    "                return recovered_obj\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load recovered portion: {e}\")\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Recovery attempt failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def try_alternative_pickle_methods(file_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Try different pickle loading methods that might work around corruption\n",
    "    \"\"\"\n",
    "    print(\"Trying alternative pickle loading methods...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    methods = [\n",
    "        (\"Standard pickle.load\", lambda f: pickle.load(f)),\n",
    "        (\"Pickle with protocol 0\", lambda f: pickle.load(f)),\n",
    "        (\"Pickle with ignore_errors\", lambda f: pickle.load(f))\n",
    "    ]\n",
    "    \n",
    "    for method_name, load_func in methods:\n",
    "        try:\n",
    "            print(f\"Trying: {method_name}\")\n",
    "            with open(file_path, 'rb') as f:\n",
    "                result = load_func(f)\n",
    "            print(f\"‚úÖ Success with {method_name}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {method_name} failed: {type(e).__name__}: {str(e)[:100]}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "def analyze_corruption_pattern(file_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Analyze the corruption pattern to understand what went wrong\n",
    "    \"\"\"\n",
    "    print(\"Analyzing corruption pattern...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = f.read()\n",
    "        \n",
    "        # Look for tab characters (0x09) which caused the error\n",
    "        tab_positions = []\n",
    "        for i, byte in enumerate(data):\n",
    "            if byte == 0x09:  # Tab character\n",
    "                tab_positions.append(i)\n",
    "        \n",
    "        if tab_positions:\n",
    "            print(f\"Found {len(tab_positions)} tab characters at positions:\")\n",
    "            for i, pos in enumerate(tab_positions[:10]):  # Show first 10\n",
    "                print(f\"  Position {pos}: context = {data[max(0,pos-10):pos+10]}\")\n",
    "                if i >= 9 and len(tab_positions) > 10:\n",
    "                    print(f\"  ... and {len(tab_positions)-10} more\")\n",
    "                    break\n",
    "        else:\n",
    "            print(\"No tab characters found in file\")\n",
    "        \n",
    "        # Check for other common corruption patterns\n",
    "        null_bytes = data.count(0x00)\n",
    "        print(f\"Null bytes in file: {null_bytes}\")\n",
    "        \n",
    "        # Look for repeated patterns that might indicate corruption\n",
    "        pattern_analysis = {}\n",
    "        for pattern_len in [4, 8, 16]:\n",
    "            for i in range(0, min(1000, len(data) - pattern_len)):\n",
    "                pattern = data[i:i+pattern_len]\n",
    "                if pattern in pattern_analysis:\n",
    "                    pattern_analysis[pattern] += 1\n",
    "                else:\n",
    "                    pattern_analysis[pattern] = 1\n",
    "        \n",
    "        # Show most common patterns\n",
    "        common_patterns = sorted(pattern_analysis.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        if common_patterns:\n",
    "            print(\"Most common byte patterns:\")\n",
    "            for pattern, count in common_patterns:\n",
    "                if count > 5:  # Only show if repeated more than 5 times\n",
    "                    print(f\"  {pattern.hex()}: {count} occurrences\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Analysis failed: {e}\")\n",
    "\n",
    "def create_minimal_working_model():\n",
    "    \"\"\"\n",
    "    Create a minimal working model that can actually make predictions\n",
    "    \"\"\"\n",
    "    print(\"Creating minimal working model...\")\n",
    "    \n",
    "    try:\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create a simple trained model\n",
    "        # Generate some dummy training data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 1000\n",
    "        \n",
    "        # Features: time_diff, distance, device_type, is_attack_ip, login_successful, latency\n",
    "        X = np.random.rand(n_samples, 6)\n",
    "        X[:, 2] = np.random.randint(0, 3, n_samples)  # device_type (0,1,2)\n",
    "        X[:, 3] = np.random.randint(0, 2, n_samples)  # is_attack_ip (0,1)\n",
    "        X[:, 4] = np.random.randint(0, 2, n_samples)  # login_successful (0,1)\n",
    "        \n",
    "        # Create target based on simple rules (this makes it a realistic model)\n",
    "        y = np.zeros(n_samples)\n",
    "        y[(X[:, 1] > 0.8) | (X[:, 3] == 1) | (X[:, 4] == 0)] = 1  # High risk conditions\n",
    "        \n",
    "        # Train model\n",
    "        model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        model.fit(X_scaled, y)\n",
    "        \n",
    "        # Test the model\n",
    "        test_features = np.array([[24, 1500, 0, 0, 1, 100]]).reshape(1, -1)\n",
    "        test_scaled = scaler.transform(test_features)\n",
    "        prediction = model.predict_proba(test_scaled)[0][1]\n",
    "        \n",
    "        print(f\"Model trained successfully. Test prediction: {prediction:.3f}\")\n",
    "        \n",
    "        # Save the working model\n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'feature_columns': ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency'],\n",
    "            'model_info': {\n",
    "                'type': 'RandomForestClassifier',\n",
    "                'n_estimators': 10,\n",
    "                'features': 6,\n",
    "                'training_samples': n_samples,\n",
    "                'created': '2024-01-01'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save as working model\n",
    "        with open('bantai_model_working.pkl', 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "        \n",
    "        print(\"‚úÖ Working model saved as 'bantai_model_working.pkl'\")\n",
    "        return model_data\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"‚ùå sklearn not available, cannot create working model\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating working model: {e}\")\n",
    "        return None\n",
    "\n",
    "def comprehensive_model_fix():\n",
    "    \"\"\"\n",
    "    Comprehensive approach to fix the model loading issue\n",
    "    \"\"\"\n",
    "    print(\"üîß Comprehensive Model Fix Procedure\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Analyze the corruption\n",
    "    print(\"\\n1. Analyzing corruption pattern...\")\n",
    "    analyze_corruption_pattern()\n",
    "    \n",
    "    # Step 2: Try alternative loading methods\n",
    "    print(\"\\n2. Trying alternative loading methods...\")\n",
    "    recovered_model = try_alternative_pickle_methods()\n",
    "    \n",
    "    if recovered_model:\n",
    "        print(\"‚úÖ Successfully recovered model using alternative method!\")\n",
    "        return recovered_model\n",
    "    \n",
    "    # Step 3: Attempt partial recovery\n",
    "    print(\"\\n3. Attempting partial recovery...\")\n",
    "    partial_model = attempt_partial_recovery()\n",
    "    \n",
    "    if partial_model:\n",
    "        print(\"‚úÖ Partially recovered model data!\")\n",
    "        return partial_model\n",
    "    \n",
    "    # Step 4: Create working replacement\n",
    "    print(\"\\n4. Creating working replacement model...\")\n",
    "    working_model = create_minimal_working_model()\n",
    "    \n",
    "    if working_model:\n",
    "        print(\"‚úÖ Created working replacement model!\")\n",
    "        return working_model\n",
    "    \n",
    "    print(\"‚ùå All recovery methods failed\")\n",
    "    return None\n",
    "\n",
    "# Updated BantAI class that can use the working model\n",
    "class BantAI_TravelAware_Fixed:\n",
    "    \"\"\"\n",
    "    Fixed version that handles model loading issues gracefully\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file=\"geocache.json\", ml_model_path=\"bantai_model.pkl\"):\n",
    "        self.cache_file = cache_file\n",
    "        self.ml_model_path = ml_model_path\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        \n",
    "        # Load model with comprehensive error handling\n",
    "        self.load_model_robust()\n",
    "    \n",
    "    def load_model_robust(self):\n",
    "        \"\"\"\n",
    "        Robust model loading with fallback options\n",
    "        \"\"\"\n",
    "        # Try original file first\n",
    "        if self._try_load_model(self.ml_model_path):\n",
    "            print(f\"‚úÖ Loaded model from {self.ml_model_path}\")\n",
    "            return True\n",
    "        \n",
    "        # Try working model file\n",
    "        working_path = \"bantai_model_working.pkl\"\n",
    "        if self._try_load_model(working_path):\n",
    "            print(f\"‚úÖ Loaded working model from {working_path}\")\n",
    "            return True\n",
    "        \n",
    "        # Try to fix and create working model\n",
    "        print(\"üîß Attempting to create working model...\")\n",
    "        working_model = create_minimal_working_model()\n",
    "        if working_model and self._try_load_model(working_path):\n",
    "            print(\"‚úÖ Created and loaded working model\")\n",
    "            return True\n",
    "        \n",
    "        print(\"‚ö†Ô∏è Running without ML model - using rule-based assessment\")\n",
    "        return False\n",
    "    \n",
    "    def _try_load_model(self, file_path):\n",
    "        \"\"\"Try to load model from specific file\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(file_path):\n",
    "                return False\n",
    "                \n",
    "            with open(file_path, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "            \n",
    "            if isinstance(model_data, dict):\n",
    "                self.model = model_data.get('model')\n",
    "                self.scaler = model_data.get('scaler')\n",
    "                self.feature_columns = model_data.get('feature_columns')\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path}: {type(e).__name__}\")\n",
    "            return False\n",
    "    \n",
    "    def predict_risk(self, features):\n",
    "        \"\"\"Make risk prediction with the loaded model\"\"\"\n",
    "        if self.model is None:\n",
    "            return 0.5  # Default risk\n",
    "        \n",
    "        try:\n",
    "            if self.scaler:\n",
    "                features_scaled = self.scaler.transform([features])\n",
    "            else:\n",
    "                features_scaled = [features]\n",
    "            \n",
    "            risk_prob = self.model.predict_proba(features_scaled)[0][1]\n",
    "            return risk_prob\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return 0.5\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run comprehensive fix\n",
    "    result = comprehensive_model_fix()\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\nüéØ Testing fixed model system...\")\n",
    "        bantai_fixed = BantAI_TravelAware_Fixed()\n",
    "        \n",
    "        # Test prediction\n",
    "        test_features = [24, 1500, 0, 0, 1, 100]\n",
    "        risk = bantai_fixed.predict_risk(test_features)\n",
    "        print(f\"Risk prediction: {risk:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BantAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
