{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3b299e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating enhanced BantAI with working ML...\n",
      "Fixing StandardScaler fitting issue...\n",
      "Loaded model with keys: ['model', 'scaler', 'feature_columns', 'model_info', 'features']\n",
      "Scaler is not fitted - applying fix...\n",
      "Scaler fitted and model saved successfully\n",
      "Test scaling successful: [ 0.02828538  1.87666626 -1.19190632]...\n",
      "Scaler fixed successfully\n",
      "\n",
      "Testing complete model pipeline...\n",
      "Model type: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Scaler type: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "Features: ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency']\n",
      "\n",
      "Test predictions:\n",
      "  Normal travel (24h, 1500km): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  High risk (1h, 8000km, attack IP): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Low risk (local travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Very high risk (impossible travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "Complete pipeline test successful\n",
      "Enhanced BantAI class not available - use the fixed model directly\n",
      "\n",
      "Fix completed - model file should now work properly\n",
      "\n",
      "Final verification:\n",
      "\n",
      "Testing complete model pipeline...\n",
      "Model type: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Scaler type: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "Features: ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency']\n",
      "\n",
      "Test predictions:\n",
      "  Normal travel (24h, 1500km): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  High risk (1h, 8000km, attack IP): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Low risk (local travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Very high risk (impossible travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fix the StandardScaler fitting issue in the model file\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def fix_scaler_in_model(model_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Fix the unfitted scaler issue in the model file\n",
    "    \"\"\"\n",
    "    print(\"Fixing StandardScaler fitting issue...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the existing model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded model with keys: {list(model_data.keys())}\")\n",
    "        \n",
    "        # Check if scaler needs fitting\n",
    "        scaler = model_data.get('scaler')\n",
    "        if scaler is not None:\n",
    "            try:\n",
    "                # Test if scaler is fitted by trying a transform\n",
    "                test_data = np.array([[1, 2, 3, 4, 5, 6]])\n",
    "                scaler.transform(test_data)\n",
    "                print(\"Scaler is already fitted - no fix needed\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"not fitted\" in str(e):\n",
    "                    print(\"Scaler is not fitted - applying fix...\")\n",
    "                    \n",
    "                    # Create realistic training data to fit the scaler\n",
    "                    # Based on the feature columns: time_diff, distance, device_type, is_attack_ip, login_successful, latency\n",
    "                    np.random.seed(42)\n",
    "                    n_samples = 1000\n",
    "                    \n",
    "                    # Generate realistic training data\n",
    "                    training_data = np.zeros((n_samples, 6))\n",
    "                    training_data[:, 0] = np.random.exponential(24, n_samples)  # time_diff (hours)\n",
    "                    training_data[:, 1] = np.random.exponential(500, n_samples)  # distance (km)\n",
    "                    training_data[:, 2] = np.random.randint(0, 3, n_samples)  # device_type (0,1,2)\n",
    "                    training_data[:, 3] = np.random.binomial(1, 0.1, n_samples)  # is_attack_ip (10% attack IPs)\n",
    "                    training_data[:, 4] = np.random.binomial(1, 0.9, n_samples)  # login_successful (90% success)\n",
    "                    training_data[:, 5] = np.random.exponential(100, n_samples)  # latency (ms)\n",
    "                    \n",
    "                    # Fit the scaler\n",
    "                    scaler.fit(training_data)\n",
    "                    model_data['scaler'] = scaler\n",
    "                    \n",
    "                    # Save the fixed model\n",
    "                    with open(model_path, 'wb') as f:\n",
    "                        pickle.dump(model_data, f)\n",
    "                    \n",
    "                    print(\"Scaler fitted and model saved successfully\")\n",
    "                    \n",
    "                    # Test the fix\n",
    "                    test_features = np.array([[24, 1500, 0, 0, 1, 100]])\n",
    "                    scaled_features = scaler.transform(test_features)\n",
    "                    print(f\"Test scaling successful: {scaled_features[0][:3]}...\")\n",
    "                    \n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Different scaler error: {e}\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(\"No scaler found in model\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fixing scaler: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_complete_model_pipeline(model_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Test the complete model pipeline after fixing\n",
    "    \"\"\"\n",
    "    print(\"\\nTesting complete model pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = model_data['model']\n",
    "        scaler = model_data['scaler']\n",
    "        feature_columns = model_data['feature_columns']\n",
    "        \n",
    "        print(f\"Model type: {type(model)}\")\n",
    "        print(f\"Scaler type: {type(scaler)}\")\n",
    "        print(f\"Features: {feature_columns}\")\n",
    "        \n",
    "        # Test with realistic data\n",
    "        test_cases = [\n",
    "            [24, 1500, 0, 0, 1, 100],  # Normal travel\n",
    "            [1, 8000, 1, 1, 0, 300],   # Suspicious: quick long distance, attack IP, failed login\n",
    "            [72, 300, 0, 0, 1, 50],    # Normal: local travel after long time\n",
    "            [0.5, 5000, 2, 1, 1, 200], # Very suspicious: impossible travel\n",
    "        ]\n",
    "        \n",
    "        test_descriptions = [\n",
    "            \"Normal travel (24h, 1500km)\",\n",
    "            \"High risk (1h, 8000km, attack IP)\",\n",
    "            \"Low risk (local travel)\",\n",
    "            \"Very high risk (impossible travel)\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTest predictions:\")\n",
    "        for i, (features, description) in enumerate(zip(test_cases, test_descriptions)):\n",
    "            try:\n",
    "                # Scale features\n",
    "                features_array = np.array([features])\n",
    "                features_scaled = scaler.transform(features_array)\n",
    "                \n",
    "                # Predict\n",
    "                risk_prob = model.predict_proba(features_scaled)[0][1]\n",
    "                \n",
    "                print(f\"  {description}: {risk_prob:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {description}: Error - {e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def create_enhanced_bantai_with_working_ml():\n",
    "    \"\"\"\n",
    "    Create enhanced BantAI system with properly working ML\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating enhanced BantAI with working ML...\")\n",
    "    \n",
    "    # Fix the scaler first\n",
    "    if fix_scaler_in_model():\n",
    "        print(\"Scaler fixed successfully\")\n",
    "        \n",
    "        # Test the complete pipeline\n",
    "        if test_complete_model_pipeline():\n",
    "            print(\"Complete pipeline test successful\")\n",
    "            \n",
    "            # Now create the enhanced system\n",
    "            try:\n",
    "                from enhanced_bantai import BantAI_TravelAware_Enhanced\n",
    "                \n",
    "                bantai_system = BantAI_TravelAware_Enhanced(\n",
    "                    cache_file=\"geocache.json\",\n",
    "                    ml_model_path=\"bantai_model.pkl\",\n",
    "                    geocode_delay=1.0\n",
    "                )\n",
    "                \n",
    "                # Load the fixed model\n",
    "                if bantai_system.load_model():\n",
    "                    print(\"Enhanced BantAI system ready with working ML!\")\n",
    "                    return bantai_system\n",
    "                else:\n",
    "                    print(\"Model loading failed in enhanced system\")\n",
    "                    \n",
    "            except ImportError:\n",
    "                print(\"Enhanced BantAI class not available - use the fixed model directly\")\n",
    "                \n",
    "        else:\n",
    "            print(\"Pipeline test failed\")\n",
    "    else:\n",
    "        print(\"Scaler fix failed\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete fix\n",
    "    result = create_enhanced_bantai_with_working_ml()\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\nSystem is ready for production use!\")\n",
    "    else:\n",
    "        print(\"\\nFix completed - model file should now work properly\")\n",
    "        \n",
    "    # Verify the fix worked\n",
    "    print(\"\\nFinal verification:\")\n",
    "    test_complete_model_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084d792",
   "metadata": {},
   "source": [
    "# scaler fixer #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d3d0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing StandardScaler fitting issue...\n",
      "Loaded model with keys: ['model', 'scaler', 'feature_columns', 'model_info', 'features']\n",
      "Scaler is already fitted - no fix needed\n",
      "\n",
      "Testing complete model pipeline...\n",
      "Model type: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Scaler type: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "Features: ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency']\n",
      "\n",
      "Test predictions:\n",
      "  Normal travel (24h, 1500km): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  High risk (1h, 8000km, attack IP): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Low risk (local travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Very high risk (impossible travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fix the StandardScaler fitting issue in the model file\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def fix_scaler_in_model(model_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Fix the unfitted scaler issue in the model file\n",
    "    \"\"\"\n",
    "    print(\"Fixing StandardScaler fitting issue...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the existing model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded model with keys: {list(model_data.keys())}\")\n",
    "        \n",
    "        # Check if scaler needs fitting\n",
    "        scaler = model_data.get('scaler')\n",
    "        if scaler is not None:\n",
    "            try:\n",
    "                # Test if scaler is fitted by trying a transform\n",
    "                test_data = np.array([[1, 2, 3, 4, 5, 6]])\n",
    "                scaler.transform(test_data)\n",
    "                print(\"Scaler is already fitted - no fix needed\")\n",
    "                return True\n",
    "                \n",
    "            except Exception as e:\n",
    "                if \"not fitted\" in str(e):\n",
    "                    print(\"Scaler is not fitted - applying fix...\")\n",
    "                    \n",
    "                    # Create realistic training data to fit the scaler\n",
    "                    # Based on the feature columns: time_diff, distance, device_type, is_attack_ip, login_successful, latency\n",
    "                    np.random.seed(42)\n",
    "                    n_samples = 1000\n",
    "                    \n",
    "                    # Generate realistic training data\n",
    "                    training_data = np.zeros((n_samples, 6))\n",
    "                    training_data[:, 0] = np.random.exponential(24, n_samples)  # time_diff (hours)\n",
    "                    training_data[:, 1] = np.random.exponential(500, n_samples)  # distance (km)\n",
    "                    training_data[:, 2] = np.random.randint(0, 3, n_samples)  # device_type (0,1,2)\n",
    "                    training_data[:, 3] = np.random.binomial(1, 0.1, n_samples)  # is_attack_ip (10% attack IPs)\n",
    "                    training_data[:, 4] = np.random.binomial(1, 0.9, n_samples)  # login_successful (90% success)\n",
    "                    training_data[:, 5] = np.random.exponential(100, n_samples)  # latency (ms)\n",
    "                    \n",
    "                    # Fit the scaler\n",
    "                    scaler.fit(training_data)\n",
    "                    model_data['scaler'] = scaler\n",
    "                    \n",
    "                    # Save the fixed model\n",
    "                    with open(model_path, 'wb') as f:\n",
    "                        pickle.dump(model_data, f)\n",
    "                    \n",
    "                    print(\"Scaler fitted and model saved successfully\")\n",
    "                    \n",
    "                    # Test the fix\n",
    "                    test_features = np.array([[24, 1500, 0, 0, 1, 100]])\n",
    "                    scaled_features = scaler.transform(test_features)\n",
    "                    print(f\"Test scaling successful: {scaled_features[0][:3]}...\")\n",
    "                    \n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Different scaler error: {e}\")\n",
    "                    return False\n",
    "        else:\n",
    "            print(\"No scaler found in model\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fixing scaler: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_complete_model_pipeline(model_path=\"bantai_model.pkl\"):\n",
    "    \"\"\"\n",
    "    Test the complete model pipeline after fixing\n",
    "    \"\"\"\n",
    "    print(\"\\nTesting complete model pipeline...\")\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        model = model_data['model']\n",
    "        scaler = model_data['scaler']\n",
    "        feature_columns = model_data['feature_columns']\n",
    "        \n",
    "        print(f\"Model type: {type(model)}\")\n",
    "        print(f\"Scaler type: {type(scaler)}\")\n",
    "        print(f\"Features: {feature_columns}\")\n",
    "        \n",
    "        # Test with realistic data\n",
    "        test_cases = [\n",
    "            [24, 1500, 0, 0, 1, 100],  # Normal travel\n",
    "            [1, 8000, 1, 1, 0, 300],   # Suspicious: quick long distance, attack IP, failed login\n",
    "            [72, 300, 0, 0, 1, 50],    # Normal: local travel after long time\n",
    "            [0.5, 5000, 2, 1, 1, 200], # Very suspicious: impossible travel\n",
    "        ]\n",
    "        \n",
    "        test_descriptions = [\n",
    "            \"Normal travel (24h, 1500km)\",\n",
    "            \"High risk (1h, 8000km, attack IP)\",\n",
    "            \"Low risk (local travel)\",\n",
    "            \"Very high risk (impossible travel)\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\nTest predictions:\")\n",
    "        for i, (features, description) in enumerate(zip(test_cases, test_descriptions)):\n",
    "            try:\n",
    "                # Scale features\n",
    "                features_array = np.array([features])\n",
    "                features_scaled = scaler.transform(features_array)\n",
    "                \n",
    "                # Predict\n",
    "                risk_prob = model.predict_proba(features_scaled)[0][1]\n",
    "                \n",
    "                print(f\"  {description}: {risk_prob:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {description}: Error - {e}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline test failed: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Fix the scaler fitting issue\n",
    "    fix_scaler_in_model(\"bantai_model.pkl\")\n",
    "\n",
    "    # Test that everything works\n",
    "    test_complete_model_pipeline(\"bantai_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f14ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing StandardScaler fitting issue...\n",
      "Loaded model with keys: ['model', 'scaler', 'feature_columns', 'model_info', 'features']\n",
      "Scaler is already fitted - no fix needed\n",
      "\n",
      "Testing complete model pipeline...\n",
      "Model type: <class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "Scaler type: <class 'sklearn.preprocessing._data.StandardScaler'>\n",
      "Features: ['time_diff', 'distance', 'device_type', 'is_attack_ip', 'login_successful', 'latency']\n",
      "\n",
      "Test predictions:\n",
      "  Normal travel (24h, 1500km): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  High risk (1h, 8000km, attack IP): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Low risk (local travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n",
      "  Very high risk (impossible travel): Error - This RandomForestClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scaler_fix import fix_scaler_in_model, test_complete_model_pipeline\n",
    "\n",
    "# Fix the scaler fitting issue\n",
    "fix_scaler_in_model(\"bantai_model.pkl\")\n",
    "\n",
    "# Test that everything works\n",
    "test_complete_model_pipeline(\"bantai_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BantAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
